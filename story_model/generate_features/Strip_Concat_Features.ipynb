{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate HTML Extracted Features Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reads the dataset with extracted features from the HTML that were generated by ExtractFeatures_Story.ipynb and extractFeaturesFucnctions.py. It drops large sized features (e.g. text, link content, code content) and concatenates it into a single file that is compact and suitable for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_seq_items = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Reading chunk 1\n",
      "Processed 50000 rows. Processed  50000 clean rows.\n",
      "Processed 100000 rows. Processed  100000 clean rows.\n",
      "Processed 150000 rows. Processed  150000 clean rows.\n",
      "Processed 200000 rows. Processed  200000 clean rows.\n",
      "Processed 250000 rows. Processed  250000 clean rows.\n",
      "Processed 300000 rows. Processed  300000 clean rows.\n",
      "Processed 350000 rows. Processed  350000 clean rows.\n",
      "Processed 400000 rows. Processed  400000 clean rows.\n",
      "Processed 450000 rows. Processed  450000 clean rows.\n",
      "Processed 500000 rows. Processed  500000 clean rows.\n",
      "Processed 550000 rows. Processed  550000 clean rows.\n",
      "Processed 600000 rows. Processed  600000 clean rows.\n",
      "Processed 650000 rows. Processed  650000 clean rows.\n",
      "Processed 700000 rows. Processed  700000 clean rows.\n",
      "Processed 750000 rows. Processed  750000 clean rows.\n",
      "Processed 800000 rows. Processed  800000 clean rows.\n",
      "Processed 850000 rows. Processed  850000 clean rows.\n",
      "Processed 900000 rows. Processed  900000 clean rows.\n",
      "Processed 950000 rows. Processed  950000 clean rows.\n",
      "Processed 1000000 rows. Processed  1000000 clean rows.\n",
      "Processed 1050000 rows. Processed  1050000 clean rows.\n",
      "Processed 1100000 rows. Processed  1100000 clean rows.\n",
      "Processed 1150000 rows. Processed  1150000 clean rows.\n",
      "Processed 1200000 rows. Processed  1200000 clean rows.\n",
      "Processed 1250000 rows. Processed  1250000 clean rows.\n",
      "Processed 1300000 rows. Processed  1300000 clean rows.\n",
      "Processed 1350000 rows. Processed  1350000 clean rows.\n",
      "Processed 1400000 rows. Processed  1400000 clean rows.\n",
      "Processed 1450000 rows. Processed  1450000 clean rows.\n",
      "Processed 1500000 rows. Processed  1500000 clean rows.\n",
      "Processed 1550000 rows. Processed  1550000 clean rows.\n",
      "Processed 1600000 rows. Processed  1600000 clean rows.\n",
      "Processed 1650000 rows. Processed  1650000 clean rows.\n",
      "Processed 1700000 rows. Processed  1700000 clean rows.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# pd.options.display.max_colwidth = 10000\n",
    "for i in range(1,5):\n",
    "    print(i)\n",
    "# loop csv chunks 1-4\n",
    "for i in range(1,5):\n",
    "    print(\"Reading chunk\", i)\n",
    "    # # File to read from\n",
    "    filename_read = '../../data/features/df_story_features_chunk' + str(i) + '.csv'\n",
    "\n",
    "    # # File name to be write to\n",
    "    filename_write = '../../data/features_analysis/df_story_features_analysis_chunk' + str(i) + '.csv'\n",
    "\n",
    "\n",
    "    # Remove csv if exists\n",
    "#     import os\n",
    "#     if os.path.exists(filepath):\n",
    "#       os.remove(filepath)\n",
    "\n",
    "    import extractFeaturesFunctions\n",
    "\n",
    "    count = 0\n",
    "    header = True\n",
    "\n",
    "    # numRows = 10000\n",
    "    chunkSize =50000\n",
    "\n",
    "    processedRows = 0\n",
    "    chunkRows = 0\n",
    "     # Read chunks\n",
    "    # for data in pd.read_csv('../../data/features/df_story_features_chunk1.csv',encoding = 'ISO-8859-1', nrows=numRows, chunksize  = chunkSize, low_memory=False):\n",
    "     # Read entire data\n",
    "    for data in pd.read_csv(filename_read,encoding = 'ISO-8859-1', chunksize  = chunkSize, low_memory=False):\n",
    "\n",
    "        count += 1                          # counting the number of chunks\n",
    "        lastlen = len(data)                 # finding the length of last chunk\n",
    "\n",
    "    #     Dropping features unsuitable for model\n",
    "    #     data.drop(['StoryIndex','StoryTitle', 'StoryURL', 'StoryURL_Story', 'Tag1', 'Tag2', 'Tag3',\n",
    "    #        'Tag4', 'Tag5', 'TagSource', 'User', 'UserURL', 'Text', 'Tag1Clap_mean',  'CodeBlockLengthList', 'ReadabilityDifficultWordsList',  'LinkURLList', 'HLightTextList', 'Tag1Clap_median',\n",
    "    #        'Tag1Clap_count', 'Tag1Clap_sum', 'Tag1Clap_std', 'Tag1Response_mean',\n",
    "    #        'Tag1Response_median', 'Tag1Response_count', 'Tag1Response_sum',\n",
    "    #        'Tag1Response_std', 'Tag1ReadingTime_mean', 'Tag1ReadingTime_median',\n",
    "    #        'Tag1ReadingTime_count', 'Tag1ReadingTime_sum', 'Tag1ReadingTime_std',\n",
    "    #        'Tag2Clap_mean', 'Tag2Clap_median', 'Tag2Clap_count', 'Tag2Clap_sum',\n",
    "    #        'Tag2Clap_std', 'Tag2Response_mean', 'Tag2Response_median',\n",
    "    #        'Tag2Response_count', 'Tag2Response_sum', 'Tag2Response_std',\n",
    "    #        'Tag2ReadingTime_mean', 'Tag2ReadingTime_median',\n",
    "    #        'Tag2ReadingTime_count', 'Tag2ReadingTime_sum', 'Tag2ReadingTime_std',\n",
    "    #        'Tag3Clap_mean', 'Tag3Clap_median', 'Tag3Clap_count', 'Tag3Clap_sum',\n",
    "    #        'Tag3Clap_std', 'Tag3Response_mean', 'Tag3Response_median',\n",
    "    #        'Tag3Response_count', 'Tag3Response_sum', 'Tag3Response_std',\n",
    "    #        'Tag3ReadingTime_mean', 'Tag3ReadingTime_median',\n",
    "    #        'Tag3ReadingTime_count', 'Tag3ReadingTime_sum', 'Tag3ReadingTime_std',\n",
    "    #        'Tag4Clap_mean', 'Tag4Clap_median', 'Tag4Clap_count', 'Tag4Clap_sum',\n",
    "    #        'Tag4Clap_std', 'Tag4Response_mean', 'Tag4Response_median',\n",
    "    #        'Tag4Response_count', 'Tag4Response_sum', 'Tag4Response_std',\n",
    "    #        'Tag4ReadingTime_mean', 'Tag4ReadingTime_median',\n",
    "    #        'Tag4ReadingTime_count', 'Tag4ReadingTime_sum', 'Tag4ReadingTime_std',\n",
    "    #        'Tag5Clap_mean', 'Tag5Clap_median', 'Tag5Clap_count', 'Tag5Clap_sum',\n",
    "    #        'Tag5Clap_std', 'Tag5Response_mean', 'Tag5Response_median',\n",
    "    #        'Tag5Response_count', 'Tag5Response_sum', 'Tag5Response_std',\n",
    "    #        'Tag5ReadingTime_mean', 'Tag5ReadingTime_median',\n",
    "    #        'Tag5ReadingTime_count', 'Tag5ReadingTime_sum', 'Tag5ReadingTime_std', 'CodeBlockRaw', 'CodeInlineRaw', 'StoryHTML'], axis=1, inplace=True)\n",
    "\n",
    "        # # Concatenate tags for analysis\n",
    "        data['TagsConcat'] = data[['Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5']].apply(lambda x: x.str.cat(sep=','), axis=1)\n",
    "        \n",
    "        # # Dropping features unsuitable for analysis (contains text)\n",
    "\n",
    "        data.drop(['StoryIndex', 'StoryURL_Story', 'TagSource', 'UserURL', 'Tag1Clap_mean',  'CodeBlockLengthList', 'ReadabilityDifficultWordsList', 'Tag1Clap_median',\n",
    "           'Tag1Clap_count', 'Tag1Clap_sum', 'Tag1Clap_std', 'Tag1Response_mean',\n",
    "           'Tag1Response_median', 'Tag1Response_count', 'Tag1Response_sum',\n",
    "           'Tag1Response_std', 'Tag1ReadingTime_mean', 'Tag1ReadingTime_median',\n",
    "           'Tag1ReadingTime_count', 'Tag1ReadingTime_sum', 'Tag1ReadingTime_std',\n",
    "           'Tag2Clap_mean', 'Tag2Clap_median', 'Tag2Clap_count', 'Tag2Clap_sum',\n",
    "           'Tag2Clap_std', 'Tag2Response_mean', 'Tag2Response_median',\n",
    "           'Tag2Response_count', 'Tag2Response_sum', 'Tag2Response_std',\n",
    "           'Tag2ReadingTime_mean', 'Tag2ReadingTime_median',\n",
    "           'Tag2ReadingTime_count', 'Tag2ReadingTime_sum', 'Tag2ReadingTime_std',\n",
    "           'Tag3Clap_mean', 'Tag3Clap_median', 'Tag3Clap_count', 'Tag3Clap_sum',\n",
    "           'Tag3Clap_std', 'Tag3Response_mean', 'Tag3Response_median',\n",
    "           'Tag3Response_count', 'Tag3Response_sum', 'Tag3Response_std',\n",
    "           'Tag3ReadingTime_mean', 'Tag3ReadingTime_median',\n",
    "           'Tag3ReadingTime_count', 'Tag3ReadingTime_sum', 'Tag3ReadingTime_std',\n",
    "           'Tag4Clap_mean', 'Tag4Clap_median', 'Tag4Clap_count', 'Tag4Clap_sum',\n",
    "           'Tag4Clap_std', 'Tag4Response_mean', 'Tag4Response_median',\n",
    "           'Tag4Response_count', 'Tag4Response_sum', 'Tag4Response_std',\n",
    "           'Tag4ReadingTime_mean', 'Tag4ReadingTime_median',\n",
    "           'Tag4ReadingTime_count', 'Tag4ReadingTime_sum', 'Tag4ReadingTime_std',\n",
    "           'Tag5Clap_mean', 'Tag5Clap_median', 'Tag5Clap_count', 'Tag5Clap_sum',\n",
    "           'Tag5Clap_std', 'Tag5Response_mean', 'Tag5Response_median',\n",
    "           'Tag5Response_count', 'Tag5Response_sum', 'Tag5Response_std',\n",
    "           'Tag5ReadingTime_mean', 'Tag5ReadingTime_median',\n",
    "           'Tag5ReadingTime_count', 'Tag5ReadingTime_sum', 'Tag5ReadingTime_std'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        processedRows += len(data)\n",
    "        chunkRows += chunkSize\n",
    "        print(\"Processed\", chunkRows, \"rows. Processed \", processedRows, \"clean rows.\")\n",
    "\n",
    "        data.to_csv(filename_write, header=header, mode=\"a\")\n",
    "        header = False\n",
    "    datalength = (count*chunkSize + lastlen - chunkSize) # length of total file\n",
    "\n",
    "    print(datalength, \"rows read\", processedRows, \"rows clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = pd.read_csv('../../data/features_model/df_story_features_model_chunk1.csv')\n",
    "# data2 = pd.read_csv('../../data/features_model/df_story_features_model_chunk2.csv')\n",
    "# data3 = pd.read_csv('../../data/features_model/df_story_features_model_chunk3.csv')\n",
    "# data4 = pd.read_csv('../../data/features_model/df_story_features_model_chunk4.csv')\n",
    "# df_features = pd.concat([data1, data2, data3, data4])\n",
    "# df_features.to_csv(\"../../data/features_model/df_story_features_model_whole.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('../../data/features_analysis/df_story_features_analysis_chunk1.csv')\n",
    "data2 = pd.read_csv('../../data/features_analysis/df_story_features_analysis_chunk2.csv')\n",
    "data3 = pd.read_csv('../../data/features_analysis/df_story_features_analysis_chunk3.csv')\n",
    "data4 = pd.read_csv('../../data/features_analysis/df_story_features_analysis_chunk4.csv')\n",
    "df_features = pd.concat([data1, data2, data3, data4])\n",
    "df_features.to_csv(\"../../data/features_analysis/df_story_features_analysis_text_whole.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

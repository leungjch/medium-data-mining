{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "from multiprocessing import  Pool\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load author data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorStats = pd.read_csv(\"data/clean/authorStats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorStats.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tag data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TagSource', 'TagClap_mean', 'TagClap_median', 'TagClap_count',\n",
      "       'TagClap_sum', 'TagClap_std', 'TagResponse_mean', 'TagResponse_median',\n",
      "       'TagResponse_count', 'TagResponse_sum', 'TagResponse_std',\n",
      "       'TagReadingTime_mean', 'TagReadingTime_median', 'TagReadingTime_count',\n",
      "       'TagReadingTime_sum', 'TagReadingTime_std'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "tagStats = pd.read_csv(\"data/clean/tagStats.csv\")\n",
    "tagStats1 =  tagStats.copy()\n",
    "tagStats2 = tagStats.copy()\n",
    "tagStats3 =  tagStats.copy()\n",
    "tagStats4 = tagStats.copy()\n",
    "tagStats5 =  tagStats.copy()\n",
    "print(tagStats.columns)\n",
    "tagStats1.columns = ['Tag1', 'Tag1Clap_mean', 'Tag1Clap_median', 'Tag1Clap_count', 'Tag1Clap_sum', 'Tag1Clap_std', 'Tag1Response_mean', 'Tag1Response_median','Tag1Response_count', 'Tag1Response_sum', 'Tag1Response_std', 'Tag1ReadingTime_mean', 'Tag1ReadingTime_median', 'Tag1ReadingTime_count','Tag1ReadingTime_sum', 'Tag1ReadingTime_std']\n",
    "tagStats2.columns = ['Tag2', 'Tag2Clap_mean', 'Tag2Clap_median', 'Tag2Clap_count', 'Tag2Clap_sum', 'Tag2Clap_std', 'Tag2Response_mean', 'Tag2Response_median','Tag2Response_count', 'Tag2Response_sum', 'Tag2Response_std', 'Tag2ReadingTime_mean', 'Tag2ReadingTime_median', 'Tag2ReadingTime_count','Tag2ReadingTime_sum', 'Tag2ReadingTime_std']\n",
    "tagStats3.columns = ['Tag3', 'Tag3Clap_mean', 'Tag3Clap_median', 'Tag3Clap_count', 'Tag3Clap_sum', 'Tag3Clap_std', 'Tag3Response_mean', 'Tag3Response_median','Tag3Response_count', 'Tag3Response_sum', 'Tag3Response_std', 'Tag3ReadingTime_mean', 'Tag3ReadingTime_median', 'Tag3ReadingTime_count','Tag3ReadingTime_sum', 'Tag3ReadingTime_std']\n",
    "tagStats4.columns = ['Tag4', 'Tag4Clap_mean', 'Tag4Clap_median', 'Tag4Clap_count', 'Tag4Clap_sum', 'Tag4Clap_std', 'Tag4Response_mean', 'Tag4Response_median','Tag4Response_count', 'Tag4Response_sum', 'Tag4Response_std', 'Tag4ReadingTime_mean', 'Tag4ReadingTime_median', 'Tag4ReadingTime_count','Tag4ReadingTime_sum', 'Tag4ReadingTime_std']\n",
    "tagStats5.columns = ['Tag5', 'Tag5Clap_mean', 'Tag5Clap_median', 'Tag5Clap_count', 'Tag5Clap_sum', 'Tag5Clap_std', 'Tag5Response_mean', 'Tag5Response_median','Tag5Response_count', 'Tag5Response_sum', 'Tag5Response_std', 'Tag5ReadingTime_mean', 'Tag5ReadingTime_median', 'Tag5ReadingTime_count','Tag5ReadingTime_sum', 'Tag5ReadingTime_std']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagStats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load publication data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company                          False\n",
      "PublicationClap_mean             False\n",
      "PublicationClap_median           False\n",
      "PublicationClap_count            False\n",
      "PublicationClap_sum              False\n",
      "PublicationReadingTime_mean      False\n",
      "PublicationReadingTime_median    False\n",
      "PublicationReadingTime_count     False\n",
      "PublicationReadingTime_sum       False\n",
      "PublicationVoter_mean            False\n",
      "PublicationVoter_median          False\n",
      "PublicationVoter_count           False\n",
      "PublicationVoter_sum             False\n",
      "PublicationisPaywall_mean        False\n",
      "PublicationisPaywall_median      False\n",
      "PublicationisPaywall_count       False\n",
      "PublicationisPaywall_sum         False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "publicationStats = pd.read_csv(\"data/clean/publicationStats.csv\")\n",
    "publicationStats.head()\n",
    "print(publicationStats.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate feature extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50000 rows. Processed  40141 clean rows\n",
      "Processed 100000 rows. Processed  85131 clean rows\n",
      "Processed 150000 rows. Processed  126052 clean rows\n",
      "Processed 200000 rows. Processed  164925 clean rows\n",
      "Processed 250000 rows. Processed  207255 clean rows\n",
      "Processed 300000 rows. Processed  251102 clean rows\n",
      "Processed 350000 rows. Processed  292404 clean rows\n",
      "Processed 400000 rows. Processed  337356 clean rows\n",
      "Processed 450000 rows. Processed  373148 clean rows\n",
      "Processed 500000 rows. Processed  412426 clean rows\n",
      "Processed 550000 rows. Processed  453430 clean rows\n",
      "Processed 600000 rows. Processed  498331 clean rows\n",
      "Processed 650000 rows. Processed  542830 clean rows\n",
      "Processed 700000 rows. Processed  590941 clean rows\n",
      "Processed 750000 rows. Processed  637848 clean rows\n",
      "Processed 800000 rows. Processed  684334 clean rows\n",
      "Processed 850000 rows. Processed  731830 clean rows\n",
      "Processed 900000 rows. Processed  780058 clean rows\n",
      "Processed 950000 rows. Processed  823250 clean rows\n",
      "Processed 1000000 rows. Processed  854575 clean rows\n",
      "Processed 1050000 rows. Processed  901735 clean rows\n",
      "Processed 1100000 rows. Processed  948949 clean rows\n",
      "Processed 1150000 rows. Processed  997851 clean rows\n",
      "Processed 1200000 rows. Processed  1046779 clean rows\n",
      "Processed 1250000 rows. Processed  1093906 clean rows\n",
      "Processed 1300000 rows. Processed  1136361 clean rows\n",
      "Processed 1350000 rows. Processed  1180197 clean rows\n",
      "Processed 1400000 rows. Processed  1213465 clean rows\n",
      "Processed 1450000 rows. Processed  1259422 clean rows\n",
      "Processed 1500000 rows. Processed  1307972 clean rows\n",
      "Processed 1550000 rows. Processed  1347110 clean rows\n",
      "Processed 1600000 rows. Processed  1390591 clean rows\n",
      "Processed 1650000 rows. Processed  1428340 clean rows\n",
      "Processed 1700000 rows. Processed  1471767 clean rows\n",
      "Processed 1750000 rows. Processed  1512010 clean rows\n",
      "Processed 1800000 rows. Processed  1556504 clean rows\n",
      "Processed 1850000 rows. Processed  1601397 clean rows\n",
      "Processed 1900000 rows. Processed  1644267 clean rows\n",
      "Processed 1950000 rows. Processed  1683252 clean rows\n",
      "Processed 2000000 rows. Processed  1717981 clean rows\n",
      "Processed 2050000 rows. Processed  1753388 clean rows\n",
      "2037937 rows read 1753388 rows clean\n",
      "Wall time: 3h 12min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# pd.options.display.max_colwidth = 10000\n",
    "filepath = 'data/features/df_story_features_chunk2.csv'\n",
    "# Remove csv if exists\n",
    "import os\n",
    "if os.path.exists(filepath):\n",
    "  os.remove(filepath)\n",
    "\n",
    "import extractFeaturesFunctions\n",
    "\n",
    "count = 0\n",
    "header = True\n",
    "\n",
    "# numRows = 10000\n",
    "chunkSize =50000\n",
    "\n",
    "processedRows = 0\n",
    "chunkRows = 0\n",
    " # Read chunks\n",
    "# for data in pd.read_csv('data/raw/OneDrive - University Of Waterloo/df_story_chunk2.csv',encoding = 'ISO-8859-1', nrows=numRows, chunksize  = chunkSize, low_memory=False):\n",
    " # Read entire data\n",
    "for data in pd.read_csv('data/raw/OneDrive - University Of Waterloo/df_story_chunk2.csv',encoding = 'ISO-8859-1', chunksize  = chunkSize, low_memory=False):\n",
    "\n",
    "    count += 1                          # counting the number of chunks\n",
    "    lastlen = len(data)                 # finding the length of last chunk\n",
    "\n",
    "    # Filter NaN PostID\n",
    "    data = data[-data['PostID'].isnull()]      \n",
    "    \n",
    "    # Filter NaN ResponseTime\n",
    "    data = data[-data['ReadingTime'].isnull()] \n",
    "\n",
    "    # Filter stories posted after April 2020\n",
    "    data['PublishedDate'] = pd.to_datetime(data['PublishedDate']).dt.date\n",
    "    data = data[data['PublishedDate'] < pd.to_datetime(\"2020-04-01\")]\n",
    "\n",
    "    \n",
    "    # Convert responsetime \"X min read\" to X as int\n",
    "    data['ReadingTime'] = data['ReadingTime'].str.extract('(\\d+)', expand=False).astype(int) \n",
    "    \n",
    "    # Get number of tags used\n",
    "    data['TagNum'] = data[['Tag1','Tag2','Tag3','Tag4', 'Tag5']].notnull().sum(axis=1)\n",
    "\n",
    "    # Boolean, if article belongs to publication\n",
    "    data['isPublication'] = data['Company'].notnull()\n",
    "    \n",
    "    # Extract features  from HTML\n",
    "    data[['Text', 'SentimentPolarity', 'SentimentSubjectivity', 'WordNum', \"TextSyllableNum\",\n",
    "          \"TextLexiconNum\", \"TextSentenceNum\", \"ReadabilityFleschEase\", \"ReadabilitySMOG\", \"ReadabilityFleschKincaid\", \"ReadabilityColemanLiau\", \"ReadabilityARI\", \"ReadabilityDaleChall\", \"ReadabilityDifficultWordsList\", \"ReadabilityDifficultWordsNum\", \"ReadabilityLinsearWriteFormula\", \"ReadabilityGunningFog\", \"ReadabilityReadingTime\", \"ReadabilityConsensus\", \n",
    "          'HasFeaturedImage','CodeInlineRaw', 'CodeInlineNum', \n",
    "          'CodeBlockRaw', 'CodeBlockNum', \"CodeBlockLengthList\", \"CodeBlockLengthSum\", \"CodeBlockLengthMedian\", \"CodeBlockLengthMean\", \"CodeBlockLengthStd\", \"CodeBlockLengthMin\", \"CodeBlockLengthMax\",\n",
    "          'ListOlNum', 'ListOlLength', 'ListOlSum', 'ListOlMedian', 'ListOlMean', 'ListOlStd', \"ListOlMin\", 'ListOlMax',\n",
    "          'ListUlNum', 'ListUlLength', 'ListUlSum', 'ListUlMedian', 'ListUlMean', 'ListUlStd', \"ListUlMin\", 'ListUlMax',\n",
    "          'ImgNum', 'LinkURLList', 'LinkNum', 'HLightTextList', 'HlightNum', 'ParagraphNum', \"ItalicNum\", \"BoldNum\"]] = pd.DataFrame(extractFeaturesFunctions.parallelize_on_rows(data, extractFeaturesFunctions.extract_features_from_html_multiproc).tolist())\n",
    "\n",
    "#     print(type(mypd))\n",
    "    # Join author data\n",
    "    data = pd.merge(data, authorStats, on=\"User\")   \n",
    "    \n",
    "    # Join publication stats\n",
    "    \n",
    "    # PublicationClapCount: number of articles that the publication contains\n",
    "    data = pd.merge(data, tagStats1, on=\"Tag1\", how='left')\n",
    "    data = pd.merge(data, tagStats2, on=\"Tag2\", how='left')\n",
    "    data = pd.merge(data, tagStats3, on=\"Tag3\", how='left')\n",
    "    data = pd.merge(data, tagStats4, on=\"Tag4\", how='left')\n",
    "    data = pd.merge(data, tagStats5, on=\"Tag5\", how='left')\n",
    "\n",
    "    # Join publication data\n",
    "    data = pd.merge(data, publicationStats, on=\"Company\", how='left').fillna(0)\n",
    "    \n",
    "    # Sum tag uses for all tags\n",
    "    \n",
    "    # TagUseSum: total number of times that Tag1,2,3,4,5 have been used\n",
    "    # TagUseMean: mean usage number of Tag1,2,3,4,5\n",
    "    # TagUseMedian, median usage number of Tag1,2,3,4,5\n",
    "    \n",
    "    # TagClapSum: total number of claps of articles in Tag1,2,3,4,5    \n",
    "    data['TagUseSum'] = data['Tag1Clap_count'].fillna(0) + data['Tag2Clap_count'].fillna(0) + data['Tag3Clap_count'].fillna(0) + data['Tag4Clap_count'].fillna(0) + data['Tag5Clap_count'].fillna(0)\n",
    "    data['TagUseMean'] = data['TagUseSum'].fillna(0)/data['TagNum'].fillna(1)\n",
    "    data['TagUseMedian'] = data[['Tag1Clap_count', 'Tag2Clap_count', 'Tag3Clap_count', 'Tag4Clap_count', 'Tag5Clap_count']].median(axis=1)\n",
    "    \n",
    "    data['TagClapSum']  = data['Tag1Clap_sum'].fillna(0) + data['Tag2Clap_sum'].fillna(0) + data['Tag3Clap_sum'].fillna(0) + data['Tag4Clap_sum'].fillna(0) + data['Tag5Clap_sum'].fillna(0)\n",
    "    data['TagClapMean'] = data['TagClapSum'].fillna(0)/data['TagNum'].fillna(1)\n",
    "    data['TagClapMedian'] = data[['Tag1Clap_sum', 'Tag2Clap_sum', 'Tag3Clap_sum', 'Tag4Clap_sum', 'Tag5Clap_sum']].median(axis=1)\n",
    "\n",
    "    data['TagUseMean'].fillna(0, inplace=True)\n",
    "    data['TagUseMedian'].fillna(0, inplace=True)\n",
    "    data['TagClapMean'].fillna(0, inplace=True)\n",
    "    data['TagClapMedian'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Delete raw html\n",
    "    del data['StoryHTML']\n",
    "    \n",
    "    processedRows += len(data)\n",
    "    chunkRows += chunkSize\n",
    "    print(\"Processed\", chunkRows, \"rows. Processed \", processedRows, \"clean rows.\")\n",
    "    \n",
    "    data.to_csv(filepath, header=header, mode=\"a\")\n",
    "    header = False\n",
    "datalength = (count*chunkSize + lastlen - chunkSize) # length of total file\n",
    "print(datalength, \"rows read\", processedRows, \"rows clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/features/df_story_features_chunk2.csv\")\n",
    "display(HTML(test.sample(2).to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(HTML(test.merge(authorStats[['User', 'ClapCount_Story_count']], on=\"User\").sample(5).to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_html = df.iloc[547].StoryHTML\n",
    "testsoup = BeautifulSoup(test_html, 'lxml')\n",
    "print(testsoup.find_all(\"figure\", {'class':re.compile(\"/.*[^image]$/\")}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(HTML(df.head(5).to_html()))\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "print(df[df['CodeNum']>0].head(100)['StoryURL'].to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(df[df['CodeBlockLengthMean']>0]['CodeBlockLengthMean'].mean())\n",
    "df['CodeBlockLengthMean'].hist(bins=60)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

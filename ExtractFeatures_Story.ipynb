{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "from multiprocessing import  Pool\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Strip HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['ClapCount_Card', 'ClapCount_Story', 'Company', 'CompanyURL', 'PostID',\n",
    "#        'PublishedDate', 'ReadingTime', 'ResponseNum_Card', 'ResponseNum_Story',\n",
    "#        'StoryHTML', 'StoryIndex', 'StoryTitle', 'StoryURL', 'StoryURL_Story',\n",
    "#        'Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5', 'TagSource', 'User', 'UserURL',\n",
    "#        'VoterCount', 'isPaywall']\n",
    "\n",
    "\n",
    "cols = ['ClapCount_Card', 'ClapCount_Story', 'Company', 'CompanyURL', 'PostID',\n",
    "       'PublishedDate', 'ReadingTime', 'ResponseNum_Card', 'ResponseNum_Story',\n",
    "       'StoryIndex', 'StoryTitle', 'StoryURL', 'StoryURL_Story',\n",
    "       'Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5', 'TagSource', 'User', 'UserURL',\n",
    "       'VoterCount', 'isPaywall']\n",
    "import pandas as pd\n",
    "reader = pd.read_csv('df_story.csv', chunksize=20000)\n",
    "for chunk in reader:\n",
    "    chunk.to_csv('df_noHTML.csv', index=False, header=False, mode='a')\n",
    "\n",
    "    result = chunk\n",
    "    result['StoryHTML'] = ''\n",
    "\n",
    "    result.to_csv('df_noHTML.csv', index=False, header=False, mode='a')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noHTML = pd.read_csv(\"df_noHTML.csv\", names=cols)\n",
    "display(HTML(df_noHTML.head(5).to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and filter N/A stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/df_story_chunk1.csv\", nrows=100)\n",
    "df.head()\n",
    "df = df[-df['PostID'].isnull()]\n",
    "df = df[-df['ReadingTime'].isnull()]\n",
    "\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load author data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorStats = pd.read_csv(\"data/clean/authorStats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorStats.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tag data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TagSource', 'TagClap_mean', 'TagClap_median', 'TagClap_count',\n",
      "       'TagClap_sum', 'TagClap_std', 'TagResponse_mean', 'TagResponse_median',\n",
      "       'TagResponse_count', 'TagResponse_sum', 'TagResponse_std',\n",
      "       'TagReadingTime_mean', 'TagReadingTime_median', 'TagReadingTime_count',\n",
      "       'TagReadingTime_sum', 'TagReadingTime_std'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "tagStats = pd.read_csv(\"data/clean/tagStats.csv\")\n",
    "tagStats1 =  tagStats.copy()\n",
    "tagStats2 = tagStats.copy()\n",
    "tagStats3 =  tagStats.copy()\n",
    "tagStats4 = tagStats.copy()\n",
    "tagStats5 =  tagStats.copy()\n",
    "print(tagStats.columns)\n",
    "tagStats1.columns = ['Tag1', 'Tag1Clap_mean', 'Tag1Clap_median', 'Tag1Clap_count', 'Tag1Clap_sum', 'Tag1Clap_std', 'Tag1Response_mean', 'Tag1Response_median','Tag1Response_count', 'Tag1Response_sum', 'Tag1Response_std', 'Tag1ReadingTime_mean', 'Tag1ReadingTime_median', 'Tag1ReadingTime_count','Tag1ReadingTime_sum', 'Tag1ReadingTime_std']\n",
    "tagStats2.columns = ['Tag2', 'Tag2Clap_mean', 'Tag2Clap_median', 'Tag2Clap_count', 'Tag2Clap_sum', 'Tag2Clap_std', 'Tag2Response_mean', 'Tag2Response_median','Tag2Response_count', 'Tag2Response_sum', 'Tag2Response_std', 'Tag2ReadingTime_mean', 'Tag2ReadingTime_median', 'Tag2ReadingTime_count','Tag2ReadingTime_sum', 'Tag2ReadingTime_std']\n",
    "tagStats3.columns = ['Tag3', 'Tag3Clap_mean', 'Tag3Clap_median', 'Tag3Clap_count', 'Tag3Clap_sum', 'Tag3Clap_std', 'Tag3Response_mean', 'Tag3Response_median','Tag3Response_count', 'Tag3Response_sum', 'Tag3Response_std', 'Tag3ReadingTime_mean', 'Tag3ReadingTime_median', 'Tag3ReadingTime_count','Tag3ReadingTime_sum', 'Tag3ReadingTime_std']\n",
    "tagStats4.columns = ['Tag4', 'Tag4Clap_mean', 'Tag4Clap_median', 'Tag4Clap_count', 'Tag4Clap_sum', 'Tag4Clap_std', 'Tag4Response_mean', 'Tag4Response_median','Tag4Response_count', 'Tag4Response_sum', 'Tag4Response_std', 'Tag4ReadingTime_mean', 'Tag4ReadingTime_median', 'Tag4ReadingTime_count','Tag4ReadingTime_sum', 'Tag4ReadingTime_std']\n",
    "tagStats5.columns = ['Tag5', 'Tag5Clap_mean', 'Tag5Clap_median', 'Tag5Clap_count', 'Tag5Clap_sum', 'Tag5Clap_std', 'Tag5Response_mean', 'Tag5Response_median','Tag5Response_count', 'Tag5Response_sum', 'Tag5Response_std', 'Tag5ReadingTime_mean', 'Tag5ReadingTime_median', 'Tag5ReadingTime_count','Tag5ReadingTime_sum', 'Tag5ReadingTime_std']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagStats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load publication data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company                          False\n",
      "PublicationClap_mean             False\n",
      "PublicationClap_median           False\n",
      "PublicationClap_count            False\n",
      "PublicationClap_sum              False\n",
      "PublicationReadingTime_mean      False\n",
      "PublicationReadingTime_median    False\n",
      "PublicationReadingTime_count     False\n",
      "PublicationReadingTime_sum       False\n",
      "PublicationVoter_mean            False\n",
      "PublicationVoter_median          False\n",
      "PublicationVoter_count           False\n",
      "PublicationVoter_sum             False\n",
      "PublicationisPaywall_mean        False\n",
      "PublicationisPaywall_median      False\n",
      "PublicationisPaywall_count       False\n",
      "PublicationisPaywall_sum         False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "publicationStats = pd.read_csv(\"data/clean/publicationStats.csv\")\n",
    "publicationStats.head()\n",
    "print(publicationStats.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate feature extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50000 rows. Processed  40141 clean rows\n",
      "Processed 100000 rows. Processed  85131 clean rows\n",
      "Processed 150000 rows. Processed  126052 clean rows\n",
      "Processed 200000 rows. Processed  164925 clean rows\n",
      "Processed 250000 rows. Processed  207255 clean rows\n",
      "Processed 300000 rows. Processed  251102 clean rows\n",
      "Processed 350000 rows. Processed  292404 clean rows\n",
      "Processed 400000 rows. Processed  337356 clean rows\n",
      "Processed 450000 rows. Processed  373148 clean rows\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# pd.options.display.max_colwidth = 10000\n",
    "filepath = 'data/features/df_story_features_chunk2.csv'\n",
    "# Remove csv if exists\n",
    "import os\n",
    "if os.path.exists(filepath):\n",
    "  os.remove(filepath)\n",
    "\n",
    "import extractFeaturesFunction\n",
    "\n",
    "count = 0\n",
    "header = True\n",
    "\n",
    "# numRows = 10000\n",
    "chunkSize =50000\n",
    "\n",
    "processedRows = 0\n",
    "chunkRows = 0\n",
    " # Read chunks\n",
    "# for data in pd.read_csv('data/raw/OneDrive - University Of Waterloo/df_story_chunk2.csv',encoding = 'ISO-8859-1', nrows=numRows, chunksize  = chunkSize, low_memory=False):\n",
    " # Read entire data\n",
    "for data in pd.read_csv('data/raw/OneDrive - University Of Waterloo/df_story_chunk2.csv',encoding = 'ISO-8859-1', chunksize  = chunkSize, low_memory=False):\n",
    "\n",
    "    count += 1                          # counting the number of chunks\n",
    "    lastlen = len(data)                 # finding the length of last chunk\n",
    "\n",
    "    # Filter NaN PostID\n",
    "    data = data[-data['PostID'].isnull()]      \n",
    "    \n",
    "    # Filter NaN ResponseTime\n",
    "    data = data[-data['ReadingTime'].isnull()] \n",
    "\n",
    "    # Filter stories posted after April 2020\n",
    "    data['PublishedDate'] = pd.to_datetime(data['PublishedDate']).dt.date\n",
    "    data = data[data['PublishedDate'] < pd.to_datetime(\"2020-04-01\")]\n",
    "\n",
    "    \n",
    "    # Convert responsetime \"X min read\" to X as int\n",
    "    data['ReadingTime'] = data['ReadingTime'].str.extract('(\\d+)', expand=False).astype(int) \n",
    "    \n",
    "    # Get number of tags used\n",
    "    data['TagNum'] = data[['Tag1','Tag2','Tag3','Tag4', 'Tag5']].notnull().sum(axis=1)\n",
    "\n",
    "    # Boolean, if article belongs to publication\n",
    "    data['isPublication'] = data['Company'].notnull()\n",
    "    \n",
    "    # Extract features  from HTML\n",
    "    data[['Text', 'SentimentPolarity', 'SentimentSubjectivity', 'WordNum', \"TextSyllableNum\",\n",
    "          \"TextLexiconNum\", \"TextSentenceNum\", \"ReadabilityFleschEase\", \"ReadabilitySMOG\", \"ReadabilityFleschKincaid\", \"ReadabilityColemanLiau\", \"ReadabilityARI\", \"ReadabilityDaleChall\", \"ReadabilityDifficultWordsList\", \"ReadabilityDifficultWordsNum\", \"ReadabilityLinsearWriteFormula\", \"ReadabilityGunningFog\", \"ReadabilityReadingTime\", \"ReadabilityConsensus\", \n",
    "          'HasFeaturedImage','CodeInlineRaw', 'CodeInlineNum', \n",
    "          'CodeBlockRaw', 'CodeBlockNum', \"CodeBlockLengthList\", \"CodeBlockLengthSum\", \"CodeBlockLengthMedian\", \"CodeBlockLengthMean\", \"CodeBlockLengthStd\", \"CodeBlockLengthMin\", \"CodeBlockLengthMax\",\n",
    "          'ListOlNum', 'ListOlLength', 'ListOlSum', 'ListOlMedian', 'ListOlMean', 'ListOlStd', \"ListOlMin\", 'ListOlMax',\n",
    "          'ListUlNum', 'ListUlLength', 'ListUlSum', 'ListUlMedian', 'ListUlMean', 'ListUlStd', \"ListUlMin\", 'ListUlMax',\n",
    "          'ImgNum', 'LinkURLList', 'LinkNum', 'HLightTextList', 'HlightNum', 'ParagraphNum', \"ItalicNum\", \"BoldNum\"]] = pd.DataFrame(extractFeaturesFunction.parallelize_on_rows(data, extractFeaturesFunction.extract_features_from_html_multiproc).tolist())\n",
    "\n",
    "#     print(type(mypd))\n",
    "    # Join author data\n",
    "    data = pd.merge(data, authorStats, on=\"User\")   \n",
    "    \n",
    "    # Join publication stats\n",
    "    \n",
    "    # PublicationClapCount: number of articles that the publication contains\n",
    "    data = pd.merge(data, tagStats1, on=\"Tag1\", how='left')\n",
    "    data = pd.merge(data, tagStats2, on=\"Tag2\", how='left')\n",
    "    data = pd.merge(data, tagStats3, on=\"Tag3\", how='left')\n",
    "    data = pd.merge(data, tagStats4, on=\"Tag4\", how='left')\n",
    "    data = pd.merge(data, tagStats5, on=\"Tag5\", how='left')\n",
    "\n",
    "    # Join publication data\n",
    "    data = pd.merge(data, publicationStats, on=\"Company\", how='left').fillna(0)\n",
    "    \n",
    "    # Sum tag uses for all tags\n",
    "    \n",
    "    # TagUseSum: total number of times that Tag1,2,3,4,5 have been used\n",
    "    # TagUseMean: mean usage number of Tag1,2,3,4,5\n",
    "    # TagUseMedian, median usage number of Tag1,2,3,4,5\n",
    "    \n",
    "    # TagClapSum: total number of claps of articles in Tag1,2,3,4,5    \n",
    "    data['TagUseSum'] = data['Tag1Clap_count'].fillna(0) + data['Tag2Clap_count'].fillna(0) + data['Tag3Clap_count'].fillna(0) + data['Tag4Clap_count'].fillna(0) + data['Tag5Clap_count'].fillna(0)\n",
    "    data['TagUseMean'] = data['TagUseSum'].fillna(0)/data['TagNum'].fillna(1)\n",
    "    data['TagUseMedian'] = data[['Tag1Clap_count', 'Tag2Clap_count', 'Tag3Clap_count', 'Tag4Clap_count', 'Tag5Clap_count']].median(axis=1)\n",
    "    \n",
    "    data['TagClapSum']  = data['Tag1Clap_sum'].fillna(0) + data['Tag2Clap_sum'].fillna(0) + data['Tag3Clap_sum'].fillna(0) + data['Tag4Clap_sum'].fillna(0) + data['Tag5Clap_sum'].fillna(0)\n",
    "    data['TagClapMean'] = data['TagClapSum'].fillna(0)/data['TagNum'].fillna(1)\n",
    "    data['TagClapMedian'] = data[['Tag1Clap_sum', 'Tag2Clap_sum', 'Tag3Clap_sum', 'Tag4Clap_sum', 'Tag5Clap_sum']].median(axis=1)\n",
    "\n",
    "    data['TagUseMean'].fillna(0, inplace=True)\n",
    "    data['TagUseMedian'].fillna(0, inplace=True)\n",
    "    data['TagClapMean'].fillna(0, inplace=True)\n",
    "    data['TagClapMedian'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Delete raw html\n",
    "    del data['StoryHTML']\n",
    "    \n",
    "    processedRows += len(data)\n",
    "    chunkRows += chunkSize\n",
    "    print(\"Processed\", chunkRows, \"rows. Processed \", processedRows, \"clean rows.\")\n",
    "    \n",
    "    data.to_csv(filepath, header=header, mode=\"a\")\n",
    "    header = False\n",
    "datalength = (count*chunkSize + lastlen - chunkSize) # length of total file\n",
    "print(datalength, \"rows read\", processedRows, \"rows clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/features/df_story_features_chunk2.csv\")\n",
    "display(HTML(test.sample(2).to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(HTML(test.merge(authorStats[['User', 'ClapCount_Story_count']], on=\"User\").sample(5).to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_html = df.iloc[547].StoryHTML\n",
    "testsoup = BeautifulSoup(test_html, 'lxml')\n",
    "print(testsoup.find_all(\"figure\", {'class':re.compile(\"/.*[^image]$/\")}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(HTML(df.head(5).to_html()))\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "print(df[df['CodeNum']>0].head(100)['StoryURL'].to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(df[df['CodeBlockLengthMean']>0]['CodeBlockLengthMean'].mean())\n",
    "df['CodeBlockLengthMean'].hist(bins=60)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

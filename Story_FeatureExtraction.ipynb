{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Strip HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['ClapCount_Card', 'ClapCount_Story', 'Company', 'CompanyURL', 'PostID',\n",
    "#        'PublishedDate', 'ReadingTime', 'ResponseNum_Card', 'ResponseNum_Story',\n",
    "#        'StoryHTML', 'StoryIndex', 'StoryTitle', 'StoryURL', 'StoryURL_Story',\n",
    "#        'Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5', 'TagSource', 'User', 'UserURL',\n",
    "#        'VoterCount', 'isPaywall']\n",
    "\n",
    "\n",
    "cols = ['ClapCount_Card', 'ClapCount_Story', 'Company', 'CompanyURL', 'PostID',\n",
    "       'PublishedDate', 'ReadingTime', 'ResponseNum_Card', 'ResponseNum_Story',\n",
    "       'StoryIndex', 'StoryTitle', 'StoryURL', 'StoryURL_Story',\n",
    "       'Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5', 'TagSource', 'User', 'UserURL',\n",
    "       'VoterCount', 'isPaywall']\n",
    "import pandas as pd\n",
    "reader = pd.read_csv('df_story.csv', chunksize=20000)\n",
    "for chunk in reader:\n",
    "    chunk.to_csv('df_noHTML.csv', index=False, header=False, mode='a')\n",
    "\n",
    "    result = chunk\n",
    "    result['StoryHTML'] = ''\n",
    "\n",
    "    result.to_csv('df_noHTML.csv', index=False, header=False, mode='a')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noHTML = pd.read_csv(\"df_noHTML.csv\", names=cols)\n",
    "display(HTML(df_noHTML.head(5).to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and filter N/A stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/df_story_chunk1.csv\", nrows=100)\n",
    "df.head()\n",
    "df = df[-df['PostID'].isnull()]\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "9995    False\n",
       "9996    False\n",
       "9997    False\n",
       "9998    False\n",
       "9999    False\n",
       "Name: PostID, Length: 10000, dtype: bool"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(HTML(df.tail().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Text from raw HTML\n",
    "def extract_features_from_html(data):\n",
    "    if pd.notnull(data['StoryHTML']):\n",
    "               \n",
    "        # Using beautifulsoup        \n",
    "        soup = BeautifulSoup(data['StoryHTML'])\n",
    "        \n",
    "        \n",
    "        # Extract num. images\n",
    "        img_count = int(len(soup.find_all(\"img\")))\n",
    "\n",
    "        \n",
    "        # Header information (author / title) is stored in <div> tag with no classname attribute\n",
    "        # First <div> tag is the article itself, remove every other <div> afterwards\n",
    "        for div in soup.find_all(\"div\", {'class':None})[1:]:\n",
    "            div.decompose()\n",
    "        \n",
    "        # Extract Text\n",
    "        text = soup.text\n",
    "        \n",
    "        # Extract num. words\n",
    "        word_count = int(len(soup.text.split()))\n",
    "        \n",
    "        # Extract code\n",
    "        # There are four ways that authors display code in a Medium article, the first two are retrievable.\n",
    "        # 1) Codeblock: Use ``` ``` which is converted into <pre> tag. Available in data.\n",
    "        # 2) Inline: <code> tag. Available in data.\n",
    "        # 3) Import a Gist file. N/A in data since Gists are embedded content which are not captured by Scrapy and would require a deeper request level.\n",
    "        # 4) Show it as an image. N/A in data since there is no distinguishing tag for \"code\" images and normal images.\n",
    "        \n",
    "        \n",
    "        # Extract num. code blocks that use the Medium ``` format (have <pre> tag)\n",
    "        codeblocks_default = soup.find_all(\"pre\")\n",
    "        code_default_count = len(codeblocks_default)\n",
    "        \n",
    "        # Get a list of the number of lines of code of each code block\n",
    "        # Every code block has number of <br> + 1 lines\n",
    "            # <pre>\n",
    "            # ....\n",
    "            # <br>\n",
    "            # ....\n",
    "            # </pre>        \n",
    "        # Then compute basic statistics\n",
    "        \n",
    "        code_default_lengths = [len(codeblock.find_all(\"br\"))+len(codeblock.find_all(\"span\")) for codeblock in codeblocks_default]\n",
    "        if len(code_default_lengths) > 0:\n",
    "            code_length_sum = sum(code_default_lengths)\n",
    "            code_length_median = statistics.median(code_default_lengths)\n",
    "            code_length_mean = statistics.mean(code_default_lengths)\n",
    "            code_length_min = min(code_default_lengths)\n",
    "            code_length_max = max(code_default_lengths)\n",
    "        else:\n",
    "            code_length_sum = 0\n",
    "            code_length_median = 0\n",
    "            code_length_mean = 0\n",
    "            code_length_min = 0\n",
    "            code_length_max = 0\n",
    "            \n",
    "            \n",
    "        # Extract number of <code> tags (inline code)\n",
    "        \n",
    "            \n",
    "        # Extract num. Gist code blocks\n",
    "        # Gist code blocks have form <figure class=\"ab cd ef ... >\"\n",
    "        # <figure> is also used to wrap images in article have form: \n",
    "            # <figure class = \"ab cd ef ... paragraph-image\", so we exclude those figure elements\n",
    "\n",
    "            \n",
    "#         gists = soup.find_all(\"figure\")\n",
    "#         if len(gists)>0:\n",
    "#             code_gist_count = len([x for x in gists if x.attrs.get(\"class\") and \"paragraph-image\" not in x.attrs.get(\"class\")])\n",
    "#         else:\n",
    "#             code_gist_count = 0\n",
    "\n",
    "        # An article has gist code blocks and default code blocks\n",
    "        code_count = code_default_count\n",
    "            \n",
    "                \n",
    "        # Extract a list of total links and their URLs\n",
    "        # We need to filter out Medium's internal links (share post, author profile, etc)\n",
    "        # These links contain the PostID, so we only collect <a> tags with href not containing PostID\n",
    "        links_all = soup.find_all(\"a\")\n",
    "        link_urls = [link.attrs.get('href') for link in links_all if str(data['PostID']) not in link.attrs.get('href')]\n",
    "        \n",
    "\n",
    "        # Extract total number of links\n",
    "        link_count = int(len(link_urls))\n",
    "        \n",
    "        # Extract number of highlights\n",
    "        highlights = soup.find_all(\"mark\")\n",
    "        highlight_count = int(len(highlights))\n",
    "\n",
    "        # Extract highlight text\n",
    "        highlights_text = [hlt.text for hlt in highlights]\n",
    "\n",
    "        # Extract image count\n",
    "        # A genuine image does not have alt (used for user logo), so we use this fact to verify image elements\n",
    "        # Get src and find number of unique URLs to find number of images\n",
    "        imgli = soup.find_all(\"img\")\n",
    "        newLi = [re.search(\"\\*.+(?![^\\.])\", img['src']).group(0).split(\".\", 1)[0] for img in imgli if (img.has_attr('src') and re.search(\"\\*.+(?![^\\.])\", img['src']) is not None)]\n",
    "        if len(newLi) == 0:\n",
    "            img_count = len(set([img.attrs['src'] for img in imgli if img.has_attr('src')]))\n",
    "        else:\n",
    "            img_count = len(set(newLi))-1\n",
    "    else:\n",
    "        text = np.NaN\n",
    "        word_count = np.NaN\n",
    "        code_count = np.NaN\n",
    "        img_count = np.NaN\n",
    "        link_count = np.NaN\n",
    "        link_urls = np.NaN\n",
    "        highlights_text = np.NaN\n",
    "        highlight_count = np.NaN\n",
    "        code_default_count = np.NaN\n",
    "        code_default_lengths = np.NaN\n",
    "        code_length_sum = np.NaN\n",
    "        code_length_median = np.NaN\n",
    "        code_length_mean = np.NaN\n",
    "        code_length_min = np.NaN\n",
    "        code_length_max = np.NaN\n",
    "        \n",
    "    return text, word_count, code_count, code_default_count, code_default_lengths, code_length_sum, code_length_median, code_length_mean, code_length_min, code_length_max, img_count, link_urls, link_count, highlights_text, highlight_count\n",
    "    \n",
    "test = df\n",
    "test[['Text', 'WordNum', 'CodeBlockNum', 'CodeBlockDefaultNum', \"CodeLengthList\", \"CodeLengthSum\", \"CodeLengthMedian\", \"CodeLengthMean\", \"CodeLengthMin\", \"CodeLengthMax\", 'ImgNum', 'LinkURLList', 'LinkNum', 'HLightTextList', 'HlightNum']] = df.apply(extract_features_from_html, axis=1, result_type=\"expand\")\n",
    "# del test['StoryHTML']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClapCount_Card</th>\n",
       "      <th>ClapCount_Story</th>\n",
       "      <th>Company</th>\n",
       "      <th>CompanyURL</th>\n",
       "      <th>PostID</th>\n",
       "      <th>PublishedDate</th>\n",
       "      <th>ReadingTime</th>\n",
       "      <th>ResponseNum_Card</th>\n",
       "      <th>ResponseNum_Story</th>\n",
       "      <th>StoryHTML</th>\n",
       "      <th>StoryIndex</th>\n",
       "      <th>StoryTitle</th>\n",
       "      <th>StoryURL</th>\n",
       "      <th>StoryURL_Story</th>\n",
       "      <th>Tag1</th>\n",
       "      <th>Tag2</th>\n",
       "      <th>Tag3</th>\n",
       "      <th>Tag4</th>\n",
       "      <th>Tag5</th>\n",
       "      <th>TagSource</th>\n",
       "      <th>User</th>\n",
       "      <th>UserURL</th>\n",
       "      <th>VoterCount</th>\n",
       "      <th>isPaywall</th>\n",
       "      <th>Text</th>\n",
       "      <th>WordNum</th>\n",
       "      <th>CodeBlockNum</th>\n",
       "      <th>CodeBlockDefaultNum</th>\n",
       "      <th>CodeLengthList</th>\n",
       "      <th>CodeLengthSum</th>\n",
       "      <th>CodeLengthMedian</th>\n",
       "      <th>CodeLengthMean</th>\n",
       "      <th>CodeLengthMin</th>\n",
       "      <th>CodeLengthMax</th>\n",
       "      <th>ImgNum</th>\n",
       "      <th>LinkURLList</th>\n",
       "      <th>LinkNum</th>\n",
       "      <th>HLightTextList</th>\n",
       "      <th>HlightNum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8bb33928ddc4</td>\n",
       "      <td>2020-04-30T07:19:12.850Z</td>\n",
       "      <td>6 min read</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[&lt;article&gt;&lt;section class=\"cm cn co cp ai cq cr r\"&gt;&lt;/section&gt;&lt;span class=\"r\"&gt;&lt;/span&gt;&lt;div&gt;&lt;div class=\"s u cs ct cu cv\"&gt;&lt;/div&gt;&lt;section class=\"cw cx cy cz da\"&gt;&lt;div class=\"n p\"&gt;&lt;div class=\"z ab ac ae af db ah ai\"&gt;&lt;div&gt;&lt;div class=\"dc dd ap ce de b df dg dh di dj dk dl dm dn do dp\" id=\"512c\"&gt;&lt;h1 class=\"de b df dq dh dr dj ds dl dt dn du ap\"&gt;A Tale of Two Binary Text Classifiers&lt;/h1&gt;&lt;/div&gt;&lt;div class=\"dv\"&gt;&lt;div class=\"n dw dx dy dz\"&gt;&lt;div class=\"o n\"&gt;&lt;div&gt;&lt;a href=\"/@leogarver?source=post_page-----8bb33928ddc4----------------------\" rel=\"noopener\"&gt;&lt;img alt=\"Leo Garver\" class=\"r ea eb ec\" height=\"48\" src=\"https://miro.medium.com/fit/c/96/96/0*jdR83TzgitFHouUH.jpg\" width=\"48\"/&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=\"ed ai r\"&gt;&lt;div class=\"n\"&gt;&lt;div style=\"flex:1\"&gt;&lt;span class=\"cd b ce cf cg ch r ap q\"&gt;&lt;div class=\"ee n o ef\"&gt;&lt;span class=\"cd eg eh cf av ei ej as at au ap\"&gt;&lt;a class=\"bx by bg bh bi bj bk bl bm bn ek bq br cb cc\" href=\"/@leogarver?source=post_page-----8bb33928ddc4----------------------\" rel=\"noopener\"&gt;Leo Garver&lt;/a&gt;&lt;/span&gt;&lt;div class=\"el r bw h\"&gt;&lt;button class=\"em ap q en eo ep eq er bn cb es et eu ev ew ex ey cd b ce ez fa ch fb fc cr fd fe bq\"&gt;Follow&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;span class=\"cd b ce cf cg ch r ci cj\"&gt;&lt;span class=\"cd eg eh cf av ei ej as at au ci\"&gt;&lt;div&gt;&lt;a class=\"bx by bg bh bi bj bk bl bm bn ek bq br cb cc\" href=\"/@leogarver/a-tale-of-two-binary-text-classifiers-8bb33928ddc4?source=post_page-----8bb33928ddc4----------------------\" rel=\"noopener\"&gt;Apr 30&lt;/a&gt; &lt;!-- --&gt;·&lt;!-- --&gt; &lt;!-- --&gt;6&lt;!-- --&gt; min read&lt;/div&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=\"n ff fg fh fi fj fk fl fm y\"&gt;&lt;div class=\"n o\"&gt;&lt;div class=\"fn r bw\"&gt;&lt;a class=\"bx by bg bh bi bj bk bl bm bn bz ca bq br cb cc\" href=\"//medium.com/p/8bb33928ddc4/share/twitter?source=post_actions_header---------------------------\" rel=\"noopener nofollow\" target=\"_blank\"&gt;&lt;svg class=\"q\" height=\"29\" width=\"29\"&gt;&lt;path d=\"M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z\"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=\"fn r bw\"&gt;&lt;button class=\"bx by bg bh bi bj bk bl bm bn bz ca bq br cb cc\"&gt;&lt;svg class=\"q\" fill=\"none\" height=\"29\" viewbox=\"0 0 29 29\" width=\"29\"&gt;&lt;path d=\"M5 6.36C5 5.61 5.63 5 6.4 5h16.2c.77 0 1.4.61 1.4 1.36v16.28c0 .75-.63 1.36-1.4 1.36H6.4c-.77 0-1.4-.6-1.4-1.36V6.36z\"&gt;&lt;/path&gt;&lt;path clip-rule=\"evenodd\" d=\"M10.76 20.9v-8.57H7.89v8.58h2.87zm-1.44-9.75c1 0 1.63-.65 1.63-1.48-.02-.84-.62-1.48-1.6-1.48-.99 0-1.63.64-1.63 1.48 0 .83.62 1.48 1.59 1.48h.01zM12.35 20.9h2.87v-4.79c0-.25.02-.5.1-.7.2-.5.67-1.04 1.46-1.04 1.04 0 1.46.8 1.46 1.95v4.59h2.87v-4.92c0-2.64-1.42-3.87-3.3-3.87-1.55 0-2.23.86-2.61 1.45h.02v-1.24h-2.87c.04.8 0 8.58 0 8.58z\" fill=\"#fff\" fill-rule=\"evenodd\"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/button&gt;&lt;/div&gt;&lt;div class=\"fn r bw\"&gt;&lt;a class=\"bx by bg bh bi bj bk bl bm bn bz ca bq br cb cc\" href=\"//medium.com/p/8bb33928ddc4/share/facebook?source=post_actions_header---------------------------\" rel=\"noopener nofollow\" target=\"_blank\"&gt;&lt;svg class=\"q\" height=\"29\" width=\"29\"&gt;&lt;path d=\"M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79\"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=\"fo r\"&gt;&lt;a class=\"bx by bg bh bi bj bk bl bm bn bz ca bq br cb cc\" href=\"https://medium.com/m/signin?operation=register&amp;amp;redirect=https%3A%2F%2Fmedium.com%2F%40leogarver%2Fa-tale-of-two-binary-text-classifiers-8bb33928ddc4&amp;amp;source=post_actions_header--------------------------bookmark_header-\" rel=\"noopener\"&gt;&lt;svg height=\"25\" viewbox=\"0 0 25 25\" width=\"25\"&gt;&lt;path d=\"M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z\" fill-rule=\"evenodd\"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=\"fp r am\"&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;p class=\"fq ge ap ce fs b ft fu gf fv fw gg fx fy gh fz ga gi gb gc gj gd cw\" id=\"fda1\"&gt;These past couple weeks, I’ve completed two large data science projects. Both of them were binary text classification models using NLP. While many of the methods I used were the same, the results were quite different I found the differences quite interesting.&lt;/p&gt;&lt;p class=\"fq ge ap ce fs b ft fu gf fv fw gg fx fy gh fz ga gi gb gc gj gd cw\" id=\"b134\"&gt;The first project was using binary text classification to take reddit posts and then try and determine which of two subreddits they belong to. The two subreddits I chose were those of two video games: League of Legends (LoL) and Teamfight Tactics(TFT) . Both of these games are built by the same developer, Riot Games. League of Legends was first released in 2009 and as of 2016, boasted 100 million monthly active players. There is a thriving esports scene devoted to it as well. The 2019 World Championship Finals had over 44 million unique viewers. With all of this success, Riot has recently built several new games that draw upon the same universe and characters as the ones built for League of Legends. The first of these to be released was Teamfight tactics, which was released in June 2019 and by September 2019 already had 33 million monthly players. In March 2020, TFT was released as a mobile app. Because of the greater complexity of LoL, it has no mobile app. Both games are changed very frequently, with new builds or patches every 2 to 4 weeks. To help determine what changes need to be made, they incorporate lots of player feedback. With multiple games that have the same characters, there are lots of comments that could apply to multiple games. The statement “Master Yi is way too powerful. Riot please nerf” could be referring to either LoL or TFT, and the response would be very different depending on which game it was talking about. With this in mind, I thought it would be useful to have a classifier that would predict which game a post was referring to.&lt;/p&gt;&lt;p class=\"fq ge ap ce fs b ft fu gf fv fw gg fx fy gh fz ga gi gb gc gj gd cw\" id=\"9fb0\"&gt;The second project I worked on tried to detect sarcasm in news headlines. To do this, I started by comparing stories from The Huffington Post (non-sarcastic) and The Onion(sarcastic). Sarcasm is something that can drastically change the meaning of a certain story, and while humans can detect the difference (at least sometimes), the task is very difficult for computers. Looking at two recent headlines regarding the same event, HuffPost reported on April 20th “&lt;a class=\"bx fe gk gl gm gn\" href=\"https://www.huffpost.com/entry/kim-jong-un-north-korea-health-surgery_n_5e9e5378c5b6b2e5b83718ce\" rel=\"noopener nofollow\" target=\"_blank\"&gt;South Korea Downplays Reports That Kim Jong Un Is In Poor Health After Surgery&lt;/a&gt;”, while the Onion reported on April 27th, that “&lt;a class=\"bx fe gk gl gm gn\" href=\"https://www.theonion.com/north-korean-media-report-kim-jong-un-in-best-health-of-1843112451\" rel=\"noopener nofollow\" target=\"_blank\"&gt;North Korean Media Report Kim Jong-Un In Best Health Of Life After Receiving Hundreds Of Heart Transplants&lt;/a&gt;”. For a human, the difference is obvious so I wondered if the experience with my previous NLP project would help me to create a machine learning model to do the same thing.&lt;/p&gt;&lt;p class=\"fq ge ap ce fs b ft fu gf fv fw gg fx fy gh fz ga gi gb gc gj gd cw\" id=\"552f\"&gt;Once I had grabbed and cleaned my data, my next step for both project was to vectorize the text data so it could be used for a model. There are two main ways of doing this (that I’m aware of). The first is CountVectorizer, which is quite simple. All it does is counts how many times each word occurs in each piece of text. The second one is TF-IDF Vectorizer, which is a bit more complicated. What it does is calculates how many times a word occurs (Term Frequency, the TF) and then scales it relative to how often the word occurs in all the other documents(IDF or Inverse Document Frequency). This gives larger values to words that are less common in the corpus as a whole. To determine which vectorizer to use, I tried both with a wide variety of hyperparameters before using a baseline logistic regression to see how they performed. The difference between the two was minimal in both cases, but the hyperparameters that worked were quite different for my two projects. In my video game project, the best results were when we only looked at individual words (rather than multi-word phrases) and when we removed a large number of stopwords, which are very common words that don’t add very much to context(I, and, the, with, for, etc.). This leads the model to only look at words that give a lot of content and context, which makes sense as we are trying to distinguish the posts based on their content. While the two games share many attributes, my hypothesis would be that some of the terms that only refer to one game would be the best predictors. For my headline analysis, these two hyperparameters were flipped. The model worked best when it looked at single words and two word combinations and when we did not remove any stopwords. When thinking about our problem, this also made sense. Since we were more looking at tone and style differences instead of content because both sites may have articles about the same news stories.&lt;/p&gt;&lt;p class=\"fq ge ap ce fs b ft fu gf fv fw gg fx fy gh fz ga gi gb gc gj gd cw\" id=\"741e\"&gt;After fitting a wide variety of models with our video game subreddit data, the best performing classifier I built was a Voting Classifier using Logistic Regression, Random Forests, and Support Vector Machines. To dig deeper into the data, I grabbed the coefficients from the logistic regression model to see which words were most predictive of each subreddit. My earlier prediction that game terms would be most predictive proved true. For TFT, some of the most predictive words were terms that only existed in that game such as: Set, Comp, Round, Star, and Unit. Mobile and phone were also strong predictors, which makes sense given that the game was released on mobile during the same time period that I grabbed posts from. The words that predicted LoL were mostly game terms as well. Lane, ADC, jungle, mid, and support were all very strong predictors since those aren’t terms used in TFT. One word that initially surprised me as a predictor was ‘years’. It made sense however, when you consider that LoL has been around for over 10 years and TFT for less than one. Therefore, if a post was talking about playing for years, or years ago something happened, it could only refer to LoL. In the end, it looked like my model performed very well on the initial problem of being able to distinguish which game a post was referencing.&lt;/p&gt;&lt;p class=\"fq ge ap ce fs b ft fu gf fv fw gg fx fy gh fz ga gi gb gc gj gd cw\" id=\"b8bd\"&gt;In my headline classification project, the best performing model was a Random Forest classifier. Again, I dug deeper into the model to find which words were most predictive. For headlines from the Onion, the three strongest predictors were profanity. If the Onion frequently has profane headlines, but HuffPost has a policy against it, this makes perfect sense. Three different two-word combinations were strong predictors as well. “Area man” and “Study finds” are two very common phrases in Onion headlines, but are probably too generic for use in real news. “Mike Pence” is also a strong predictor for the Onion. My reasoning for this is that he is too boring to be in many real news stories, but that makes him a great punchline in satire. One of the strongest predictors for HuffPost was “says he”. As I looked through some headlines, it looked like a lot of HuffPost stories were merely reporting on something someone said to another publication, leading to lots of ‘says he’ in headlines. Many of the other strong predictors were words pertaining to lgbtq issues. lgbtq, lgbt, queer, trans, and transgender were all strong predictors for HuffPost. To me, this means that the Onion follows one of the main rules of comedy: “Punch up, not down”, meaning you should only make jokes at the expense of people who have greater power/status than you instead of someone with less power/status. I think it’s a very good thing that the Onion doesn’t make fun of a group that has been so persecuted and discriminated against. Better to punch up, at say, Mike Pence. Looking at all of these results, it appears like the model did not do what I set out to do, detect sarcasm. Rather, it classified more on the style of the different sites’ headlines, as well as issues one covered, but not the other. If that were the objective, I think the model turned out well.&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/section&gt;&lt;/div&gt;&lt;/article&gt;]</td>\n",
       "      <td>517</td>\n",
       "      <td>A Tale of Two Binary Text Classifiers</td>\n",
       "      <td>https://medium.com/@leogarver/a-tale-of-two-binary-text-classifiers-8bb33928ddc4?source=tag_archive---------139-----------------------</td>\n",
       "      <td>https://medium.com/@leogarver/a-tale-of-two-binary-text-classifiers-8bb33928ddc4?source=tag_archive---------139-----------------------</td>\n",
       "      <td>data-science</td>\n",
       "      <td>nlp</td>\n",
       "      <td>machine-learning</td>\n",
       "      <td>league-of-legends</td>\n",
       "      <td>the-onion</td>\n",
       "      <td>machine-learning</td>\n",
       "      <td>Leo Garver</td>\n",
       "      <td>https://medium.com/@leogarver</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[These past couple weeks, I’ve completed two large data science projects. Both of them were binary text classification models using NLP. While many of the methods I used were the same, the results were quite different I found the differences quite interesting.The first project was using binary text classification to take reddit posts and then try and determine which of two subreddits they belong to. The two subreddits I chose were those of two video games: League of Legends (LoL) and Teamfight Tactics(TFT) . Both of these games are built by the same developer, Riot Games. League of Legends was first released in 2009 and as of 2016, boasted 100 million monthly active players. There is a thriving esports scene devoted to it as well. The 2019 World Championship Finals had over 44 million unique viewers. With all of this success, Riot has recently built several new games that draw upon the same universe and characters as the ones built for League of Legends. The first of these to be released was Teamfight tactics, which was released in June 2019 and by September 2019 already had 33 million monthly players. In March 2020, TFT was released as a mobile app. Because of the greater complexity of LoL, it has no mobile app. Both games are changed very frequently, with new builds or patches every 2 to 4 weeks. To help determine what changes need to be made, they incorporate lots of player feedback. With multiple games that have the same characters, there are lots of comments that could apply to multiple games. The statement “Master Yi is way too powerful. Riot please nerf” could be referring to either LoL or TFT, and the response would be very different depending on which game it was talking about. With this in mind, I thought it would be useful to have a classifier that would predict which game a post was referring to.The second project I worked on tried to detect sarcasm in news headlines. To do this, I started by comparing stories from The Huffington Post (non-sarcastic) and The Onion(sarcastic). Sarcasm is something that can drastically change the meaning of a certain story, and while humans can detect the difference (at least sometimes), the task is very difficult for computers. Looking at two recent headlines regarding the same event, HuffPost reported on April 20th “South Korea Downplays Reports That Kim Jong Un Is In Poor Health After Surgery”, while the Onion reported on April 27th, that “North Korean Media Report Kim Jong-Un In Best Health Of Life After Receiving Hundreds Of Heart Transplants”. For a human, the difference is obvious so I wondered if the experience with my previous NLP project would help me to create a machine learning model to do the same thing.Once I had grabbed and cleaned my data, my next step for both project was to vectorize the text data so it could be used for a model. There are two main ways of doing this (that I’m aware of). The first is CountVectorizer, which is quite simple. All it does is counts how many times each word occurs in each piece of text. The second one is TF-IDF Vectorizer, which is a bit more complicated. What it does is calculates how many times a word occurs (Term Frequency, the TF) and then scales it relative to how often the word occurs in all the other documents(IDF or Inverse Document Frequency). This gives larger values to words that are less common in the corpus as a whole. To determine which vectorizer to use, I tried both with a wide variety of hyperparameters before using a baseline logistic regression to see how they performed. The difference between the two was minimal in both cases, but the hyperparameters that worked were quite different for my two projects. In my video game project, the best results were when we only looked at individual words (rather than multi-word phrases) and when we removed a large number of stopwords, which are very common words that don’t add very much to context(I, and, the, with, for, etc.). This leads the model to only look at words that give a lot of content and context, which makes sense as we are trying to distinguish the posts based on their content. While the two games share many attributes, my hypothesis would be that some of the terms that only refer to one game would be the best predictors. For my headline analysis, these two hyperparameters were flipped. The model worked best when it looked at single words and two word combinations and when we did not remove any stopwords. When thinking about our problem, this also made sense. Since we were more looking at tone and style differences instead of content because both sites may have articles about the same news stories.After fitting a wide variety of models with our video game subreddit data, the best performing classifier I built was a Voting Classifier using Logistic Regression, Random Forests, and Support Vector Machines. To dig deeper into the data, I grabbed the coefficients from the logistic regression model to see which words were most predictive of each subreddit. My earlier prediction that game terms would be most predictive proved true. For TFT, some of the most predictive words were terms that only existed in that game such as: Set, Comp, Round, Star, and Unit. Mobile and phone were also strong predictors, which makes sense given that the game was released on mobile during the same time period that I grabbed posts from. The words that predicted LoL were mostly game terms as well. Lane, ADC, jungle, mid, and support were all very strong predictors since those aren’t terms used in TFT. One word that initially surprised me as a predictor was ‘years’. It made sense however, when you consider that LoL has been around for over 10 years and TFT for less than one. Therefore, if a post was talking about playing for years, or years ago something happened, it could only refer to LoL. In the end, it looked like my model performed very well on the initial problem of being able to distinguish which game a post was referencing.In my headline classification project, the best performing model was a Random Forest classifier. Again, I dug deeper into the model to find which words were most predictive. For headlines from the Onion, the three strongest predictors were profanity. If the Onion frequently has profane headlines, but HuffPost has a policy against it, this makes perfect sense. Three different two-word combinations were strong predictors as well. “Area man” and “Study finds” are two very common phrases in Onion headlines, but are probably too generic for use in real news. “Mike Pence” is also a strong predictor for the Onion. My reasoning for this is that he is too boring to be in many real news stories, but that makes him a great punchline in satire. One of the strongest predictors for HuffPost was “says he”. As I looked through some headlines, it looked like a lot of HuffPost stories were merely reporting on something someone said to another publication, leading to lots of ‘says he’ in headlines. Many of the other strong predictors were words pertaining to lgbtq issues. lgbtq, lgbt, queer, trans, and transgender were all strong predictors for HuffPost. To me, this means that the Onion follows one of the main rules of comedy: “Punch up, not down”, meaning you should only make jokes at the expense of people who have greater power/status than you instead of someone with less power/status. I think it’s a very good thing that the Onion doesn’t make fun of a group that has been so persecuted and discriminated against. Better to punch up, at say, Mike Pence. Looking at all of these results, it appears like the model did not do what I set out to do, detect sarcasm. Rather, it classified more on the style of the different sites’ headlines, as well as issues one covered, but not the other. If that were the objective, I think the model turned out well.]</td>\n",
       "      <td>1355</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[https://www.huffpost.com/entry/kim-jong-un-north-korea-health-surgery_n_5e9e5378c5b6b2e5b83718ce, https://www.theonion.com/north-korean-media-report-kim-jong-un-in-best-health-of-1843112451]</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(test.sample(1).to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "test_html = df.iloc[547].StoryHTML\n",
    "testsoup = BeautifulSoup(test_html, 'lxml')\n",
    "print(testsoup.find_all(\"figure\", {'class':re.compile(\"/.*[^image]$/\")}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545    https://medium.com/dataform/sending-data-from-bigquery-to-intercom-using-google-cloud-functions-c65f26906a4?source=tag_archive---------2-----------------------\n",
      "546    https://medium.com/datadriveninvestor/java-enums-what-a-special-type-276d27423a71?source=tag_archive---------0-----------------------                          \n"
     ]
    }
   ],
   "source": [
    "# display(HTML(df.head(5).to_html()))\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "print(df[df['CodeNum']>0].head(100)['StoryURL'].to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.057077070026525\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(df[df['CodeLengthMean']>0]['CodeLengthMean'].mean())\n",
    "df['CodeLengthMean'].hist(bins=60)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

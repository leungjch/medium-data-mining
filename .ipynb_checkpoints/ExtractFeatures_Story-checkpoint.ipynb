{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "from multiprocessing import  Pool\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Strip HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['ClapCount_Card', 'ClapCount_Story', 'Company', 'CompanyURL', 'PostID',\n",
    "#        'PublishedDate', 'ReadingTime', 'ResponseNum_Card', 'ResponseNum_Story',\n",
    "#        'StoryHTML', 'StoryIndex', 'StoryTitle', 'StoryURL', 'StoryURL_Story',\n",
    "#        'Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5', 'TagSource', 'User', 'UserURL',\n",
    "#        'VoterCount', 'isPaywall']\n",
    "\n",
    "\n",
    "cols = ['ClapCount_Card', 'ClapCount_Story', 'Company', 'CompanyURL', 'PostID',\n",
    "       'PublishedDate', 'ReadingTime', 'ResponseNum_Card', 'ResponseNum_Story',\n",
    "       'StoryIndex', 'StoryTitle', 'StoryURL', 'StoryURL_Story',\n",
    "       'Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5', 'TagSource', 'User', 'UserURL',\n",
    "       'VoterCount', 'isPaywall']\n",
    "import pandas as pd\n",
    "reader = pd.read_csv('df_story.csv', chunksize=20000)\n",
    "for chunk in reader:\n",
    "    chunk.to_csv('df_noHTML.csv', index=False, header=False, mode='a')\n",
    "\n",
    "    result = chunk\n",
    "    result['StoryHTML'] = ''\n",
    "\n",
    "    result.to_csv('df_noHTML.csv', index=False, header=False, mode='a')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noHTML = pd.read_csv(\"df_noHTML.csv\", names=cols)\n",
    "display(HTML(df_noHTML.head(5).to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data and filter N/A stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/df_story_chunk1.csv\", nrows=100)\n",
    "df.head()\n",
    "df = df[-df['PostID'].isnull()]\n",
    "df = df[-df['ReadingTime'].isnull()]\n",
    "\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load author data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorStats = pd.read_csv(\"data/clean/authorStats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['User', 'AuthorArticlesClap_mean', 'AuthorArticlesClap_median',\n",
       "       'AuthorArticlesClap_count', 'AuthorArticlesClap_sum',\n",
       "       'AuthorArticlesResponse_mean', 'AuthorArticlesResponse_median',\n",
       "       'AuthorArticlesResponse_count', 'AuthorArticlesResponse_sum',\n",
       "       'AuthorArticlesVoter_mean', 'AuthorArticlesVoter_median',\n",
       "       'AuthorArticlesVoter_count', 'AuthorArticlesVoter_sum'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authorStats.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tag data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TagSource', 'TagClap_mean', 'TagClap_median', 'TagClap_count',\n",
      "       'TagClap_sum', 'TagClap_std', 'TagResponse_mean', 'TagResponse_median',\n",
      "       'TagResponse_count', 'TagResponse_sum', 'TagResponse_std',\n",
      "       'TagReadingTime_mean', 'TagReadingTime_median', 'TagReadingTime_count',\n",
      "       'TagReadingTime_sum', 'TagReadingTime_std'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "tagStats = pd.read_csv(\"data/clean/tagStats.csv\")\n",
    "tagStats1 =  tagStats.copy()\n",
    "tagStats2 = tagStats.copy()\n",
    "tagStats3 =  tagStats.copy()\n",
    "tagStats4 = tagStats.copy()\n",
    "tagStats5 =  tagStats.copy()\n",
    "print(tagStats.columns)\n",
    "tagStats1.columns = ['Tag1', 'Tag1Clap_mean', 'Tag1Clap_median', 'Tag1Clap_count', 'Tag1Clap_sum', 'Tag1Clap_std', 'Tag1Response_mean', 'Tag1Response_median','Tag1Response_count', 'Tag1Response_sum', 'Tag1Response_std', 'Tag1ReadingTime_mean', 'Tag1ReadingTime_median', 'Tag1ReadingTime_count','Tag1ReadingTime_sum', 'Tag1ReadingTime_std']\n",
    "tagStats2.columns = ['Tag2', 'Tag2Clap_mean', 'Tag2Clap_median', 'Tag2Clap_count', 'Tag2Clap_sum', 'Tag2Clap_std', 'Tag2Response_mean', 'Tag2Response_median','Tag2Response_count', 'Tag2Response_sum', 'Tag2Response_std', 'Tag2ReadingTime_mean', 'Tag2ReadingTime_median', 'Tag2ReadingTime_count','Tag2ReadingTime_sum', 'Tag2ReadingTime_std']\n",
    "tagStats3.columns = ['Tag3', 'Tag3Clap_mean', 'Tag3Clap_median', 'Tag3Clap_count', 'Tag3Clap_sum', 'Tag3Clap_std', 'Tag3Response_mean', 'Tag3Response_median','Tag3Response_count', 'Tag3Response_sum', 'Tag3Response_std', 'Tag3ReadingTime_mean', 'Tag3ReadingTime_median', 'Tag3ReadingTime_count','Tag3ReadingTime_sum', 'Tag3ReadingTime_std']\n",
    "tagStats4.columns = ['Tag4', 'Tag4Clap_mean', 'Tag4Clap_median', 'Tag4Clap_count', 'Tag4Clap_sum', 'Tag4Clap_std', 'Tag4Response_mean', 'Tag4Response_median','Tag4Response_count', 'Tag4Response_sum', 'Tag4Response_std', 'Tag4ReadingTime_mean', 'Tag4ReadingTime_median', 'Tag4ReadingTime_count','Tag4ReadingTime_sum', 'Tag4ReadingTime_std']\n",
    "tagStats5.columns = ['Tag5', 'Tag5Clap_mean', 'Tag5Clap_median', 'Tag5Clap_count', 'Tag5Clap_sum', 'Tag5Clap_std', 'Tag5Response_mean', 'Tag5Response_median','Tag5Response_count', 'Tag5Response_sum', 'Tag5Response_std', 'Tag5ReadingTime_mean', 'Tag5ReadingTime_median', 'Tag5ReadingTime_count','Tag5ReadingTime_sum', 'Tag5ReadingTime_std']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagStats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load publication data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company                          False\n",
      "PublicationClap_mean             False\n",
      "PublicationClap_median           False\n",
      "PublicationClap_count            False\n",
      "PublicationClap_sum              False\n",
      "PublicationReadingTime_mean      False\n",
      "PublicationReadingTime_median    False\n",
      "PublicationReadingTime_count     False\n",
      "PublicationReadingTime_sum       False\n",
      "PublicationVoter_mean            False\n",
      "PublicationVoter_median          False\n",
      "PublicationVoter_count           False\n",
      "PublicationVoter_sum             False\n",
      "PublicationisPaywall_mean        False\n",
      "PublicationisPaywall_median      False\n",
      "PublicationisPaywall_count       False\n",
      "PublicationisPaywall_sum         False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "publicationStats = pd.read_csv(\"data/clean/publicationStats.csv\")\n",
    "publicationStats.head()\n",
    "print(publicationStats.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Text from raw HTML\n",
    "# Does not work with multiprocessing module, function is required to be imported from external file\n",
    "def extract_features_from_html(data):\n",
    "    if pd.notnull(data['StoryHTML']):\n",
    "               \n",
    "        # Using beautifulsoup        \n",
    "        soup = BeautifulSoup(data['StoryHTML'])\n",
    "        \n",
    "\n",
    "        \n",
    "        # Header information (author / title) is stored in <div> tag with no classname attribute\n",
    "        # It is also the second section tag\n",
    "        featured_image = False\n",
    "        \n",
    "        header = soup.find_all(\"section\") # Header is the second section\n",
    "        if len(header)>=2:\n",
    "            if header[1].find_all(\"img\", {\"role\":\"presentation\"}):\n",
    "                featured_image = True\n",
    "\n",
    "        # First <div> tag is the article itself, remove every other <div> afterwards\n",
    "        headerDivs = soup.find_all(\"div\", {'class':None})\n",
    "        if (headerDivs):  \n",
    "            for div in headerDivs[1:]:\n",
    "                div.decompose()\n",
    "        \n",
    "        # Extract num. images\n",
    "        img_count = int(len(soup.find_all(\"img\")))\n",
    "        if featured_image:\n",
    "            img_count+=1\n",
    "        \n",
    "        # Extract Text\n",
    "        text = soup.text\n",
    "        \n",
    "        if text:\n",
    "            # Textblob for sentiment\n",
    "            textBlob = TextBlob(text)\n",
    "            sentiment_polarity = textBlob.sentiment.polarity\n",
    "            sentiment_subjectivity = textBlob.sentiment.subjectivity\n",
    "        \n",
    "            TextSyllableNum = textstat.syllable_count(text)\n",
    "            TextLexiconNum = textstat.lexicon_count(text, removepunct=True)\n",
    "            TextSentenceNum = textstat.sentence_count(text)\n",
    "\n",
    "            ReadabilityFleschEase = textstat.flesch_reading_ease(text)\n",
    "            ReadabilitySMOG = textstat.smog_index(text)\n",
    "            ReadabilityFleschKincaid = textstat.flesch_kincaid_grade(text)\n",
    "            ReadabilityColemanLiau = textstat.coleman_liau_index(text)\n",
    "            ReadabilityARI = textstat.automated_readability_index(text)\n",
    "            ReadabilityDaleChall = textstat.dale_chall_readability_score(text)\n",
    "            ReadabilityDifficultWordsList = textstat.difficult_words_list(text)\n",
    "            ReadabilityDifficultWordsNum = textstat.difficult_words(text)\n",
    "            ReadabilityLinsearWriteFormula = textstat.linsear_write_formula(text)\n",
    "            ReadabilityGunningFog = textstat.gunning_fog(text)\n",
    "            ReadabilityReadingTime = textstat.reading_time(text)\n",
    "            ReadabilityConsensus = textstat.text_standard(text, float_output=True)\n",
    "\n",
    "        else:\n",
    "            sentiment_polarity = 0\n",
    "            sentiment_subjectivity = 0\n",
    "\n",
    "            TextSyllableNum = 0\n",
    "            TextLexiconNum = 0\n",
    "            TextSentenceNum = 0\n",
    "\n",
    "            ReadabilityFleschEase = 0\n",
    "            ReadabilitySMOG = 0\n",
    "            ReadabilityFleschKincaid = 0\n",
    "            ReadabilityColemanLiau = 0\n",
    "            ReadabilityARI = 0\n",
    "            ReadabilityDaleChall = 0\n",
    "            ReadabilityDifficultWordsList = 0\n",
    "            ReadabilityDifficultWordsNum = 0\n",
    "            ReadabilityLinsearWriteFormula = 0\n",
    "            ReadabilityGunningFog = 0\n",
    "            ReadabilityReadingTime = 0\n",
    "            ReadabilityConsensus = 0\n",
    "\n",
    "        \n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        if (paragraphs):\n",
    "            paragraph_num = len(paragraphs)\n",
    "        else:\n",
    "            paragraph_num = 0\n",
    "            \n",
    "            \n",
    "        # Extract bolds\n",
    "        bolds = soup.find_all(\"strong\")\n",
    "        if bolds:\n",
    "            bold_num = len(bolds)\n",
    "        else:\n",
    "            bold_num = 0\n",
    "\n",
    "        \n",
    "        # Extract italics\n",
    "        italics = soup.find_all(\"em\")\n",
    "        if italics:\n",
    "            italic_num = len(italics)\n",
    "        else:\n",
    "            italic_num = 0\n",
    "            \n",
    "        # Get unordered and ordered lists\n",
    "        lists_unordered = soup.find_all(\"ul\")\n",
    "        if lists_unordered:\n",
    "            lists_unordered_num = len(lists_unordered)\n",
    "            lists_unordered_lengths = [len(item.text) for item in lists_unordered] # the length of each item of each list\n",
    "            lists_unordered_items_sum = sum(lists_unordered_lengths)\n",
    "            lists_unordered_items_median = statistics.median(lists_unordered_lengths)\n",
    "            lists_unordered_items_mean = statistics.mean(lists_unordered_lengths)\n",
    "            lists_unordered_items_std = np.std(lists_unordered_lengths)\n",
    "            lists_unordered_items_min = min(lists_unordered_lengths)\n",
    "            lists_unordered_items_max = max(lists_unordered_lengths)\n",
    "        else:\n",
    "            lists_unordered_num = 0\n",
    "            lists_unordered_lengths = 0\n",
    "            lists_unordered_items_sum = 0\n",
    "            lists_unordered_items_median = 0\n",
    "            lists_unordered_items_mean = 0\n",
    "            lists_unordered_items_std = 0\n",
    "            lists_unordered_items_min = 0\n",
    "            lists_unordered_items_max = 0\n",
    "\n",
    "        lists_ordered = soup.find_all(\"ol\")\n",
    "        if lists_ordered:\n",
    "            lists_ordered_num = len(lists_ordered)\n",
    "            lists_ordered_lengths = [len(item.text) for item in lists_ordered]\n",
    "            lists_ordered_items_sum = sum(lists_ordered_lengths)\n",
    "            lists_ordered_items_median = statistics.median(lists_ordered_lengths)\n",
    "            lists_ordered_items_mean = statistics.mean(lists_ordered_lengths)\n",
    "            lists_ordered_items_std = np.std(lists_ordered_lengths)\n",
    "            lists_ordered_items_min = min(lists_ordered_lengths)\n",
    "            lists_ordered_items_max = max(lists_ordered_lengths)\n",
    "        else:\n",
    "            lists_ordered_num = 0\n",
    "            lists_ordered_lengths = 0\n",
    "            lists_ordered_items_sum = 0\n",
    "            lists_ordered_items_median = 0\n",
    "            lists_ordered_items_mean = 0\n",
    "            lists_ordered_items_std = 0\n",
    "            lists_ordered_items_min = 0\n",
    "            lists_ordered_items_max = 0\n",
    "\n",
    "        # Extract num. words\n",
    "        word_count = int(len(soup.text.split()))\n",
    "        \n",
    "        # Extract code\n",
    "        # There are four ways that authors display code in a Medium article, the first two are retrievable.\n",
    "        # 1) Codeblock: Use ``` ``` which is converted into <pre> tag. Available in data.\n",
    "        # 2) Inline: <code> tag. Available in data.\n",
    "        # 3) Import a Gist file. N/A in data since Gists are embedded content which are not captured by Scrapy and would require a deeper request level.\n",
    "        # 4) Show it as an image. N/A in data since there is no distinguishing tag for \"code\" images and normal images.\n",
    "        \n",
    "        # Extract num. code blocks that use the Medium ``` format (have <pre> tag)\n",
    "        codeblocks_default = soup.find_all(\"pre\")\n",
    "\n",
    "        codeblock_raw = [code.text for code in codeblocks_default]\n",
    "        \n",
    "        codeblock_num = len(codeblocks_default)\n",
    "        \n",
    "        # Get a list of the number of lines of code of each code block\n",
    "        # Every code block has number of <br> + 1 lines\n",
    "            # <pre>\n",
    "            # ....\n",
    "            # <br>\n",
    "            # ....\n",
    "            # </pre>        \n",
    "        # Then compute basic statistics\n",
    "        \n",
    "        codeblock_lengths = [len(codeblock.find_all(\"br\"))+len(codeblock.find_all(\"span\")) for codeblock in codeblocks_default]\n",
    "        if len(codeblock_lengths) > 0:\n",
    "            codeblock_length_sum = sum(codeblock_lengths)\n",
    "            codeblock_length_median = statistics.median(codeblock_lengths)\n",
    "            codeblock_length_mean = statistics.mean(codeblock_lengths)\n",
    "            codeblock_length_std = np.std(codeblock_lengths)\n",
    "            codeblock_length_min = min(codeblock_lengths)\n",
    "            codeblock_length_max = max(codeblock_lengths)\n",
    "        else:\n",
    "            codeblock_length_sum = 0\n",
    "            codeblock_length_median = 0\n",
    "            codeblock_length_mean = 0\n",
    "            codeblock_length_std = 0\n",
    "            codeblock_length_min = 0\n",
    "            codeblock_length_max = 0\n",
    "            \n",
    "        # Extract number of <code> tags (inline code)\n",
    "        code_inline = soup.find_all(\"code\")\n",
    "        code_inline_raw = [code.text for code in code_inline]\n",
    "        code_inline_num = len(code_inline)\n",
    "\n",
    "                \n",
    "        # Extract a list of total links and their URLs\n",
    "        # We need to filter out Medium's internal links (share post, author profile, etc)\n",
    "        # These links contain the PostID, so we only collect <a> tags with href not containing PostID\n",
    "        links_all = soup.find_all(\"a\")\n",
    "        link_urls = [link.attrs.get('href') for link in links_all if str(data['PostID']) not in link.attrs.get('href')]\n",
    "        \n",
    "\n",
    "        # Extract total number of links\n",
    "        link_count = int(len(link_urls))\n",
    "        \n",
    "        # Extract number of highlights\n",
    "        highlights = soup.find_all(\"mark\")\n",
    "        highlight_count = int(len(highlights))\n",
    "\n",
    "        # Extract highlight text\n",
    "        highlights_text = [hlt.text for hlt in highlights]\n",
    "\n",
    "        # Extract image count\n",
    "        # A genuine image does not have alt (used for user logo), so we use this fact to verify image elements\n",
    "        # Get src and find number of unique URLs to find number of images\n",
    "#         imgli = soup.find_all(\"img\")\n",
    "#         newLi = [re.search(\"\\*.+(?![^\\.])\", img['src']).group(0).split(\".\", 1)[0] for img in imgli if (img.has_attr('src') and re.search(\"\\*.+(?![^\\.])\", img['src']) is not None)]\n",
    "#         if len(newLi) == 0:\n",
    "#             img_count = len(set([img.attrs['src'] for img in imgli if img.has_attr('src')]))\n",
    "#         else:\n",
    "#             img_count = len(set(newLi))-1\n",
    "    else:\n",
    "        text = np.NaN\n",
    "        sentiment_polarity = np.NaN\n",
    "        sentiment_subjectivity = np.NaN\n",
    "        \n",
    "        TextSyllableNum = np.NaN\n",
    "        TextLexiconNum = np.NaN\n",
    "        TextSentenceNum = np.NaN\n",
    "\n",
    "        ReadabilityFleschEase = np.NaN\n",
    "        ReadabilitySMOG = np.NaN\n",
    "        ReadabilityFleschKincaid = np.NaN\n",
    "        ReadabilityColemanLiau = np.NaN\n",
    "        ReadabilityARI = np.NaN\n",
    "        ReadabilityDaleChall = np.NaN\n",
    "        ReadabilityDifficultWordsList = np.NaN\n",
    "        ReadabilityDifficultWordsNum = np.NaN\n",
    "        ReadabilityLinsearWriteFormula = np.NaN\n",
    "        ReadabilityGunningFog = np.NaN\n",
    "        ReadabilityReadingTime = np.NaN\n",
    "        ReadabilityConsensus = np.NaN\n",
    "\n",
    "        word_count = np.NaN\n",
    "        code_count = np.NaN\n",
    "        \n",
    "        img_count = np.NaN\n",
    "        featured_image = np.NaN\n",
    "        \n",
    "        link_count = np.NaN\n",
    "        link_urls = np.NaN\n",
    "        \n",
    "        highlights_text = np.NaN\n",
    "        highlight_count = np.NaN\n",
    "        \n",
    "        code_inline_raw = np.NaN\n",
    "        code_inline_num = np.NaN\n",
    "        \n",
    "        codeblock_raw = np.NaN\n",
    "        codeblock_num = np.NaN\n",
    "        codeblock_lengths = np.NaN\n",
    "        codeblock_length_sum = np.NaN\n",
    "        codeblock_length_median = np.NaN\n",
    "        codeblock_length_mean = np.NaN\n",
    "        codeblock_length_std = np.NaN\n",
    "        codeblock_length_min = np.NaN\n",
    "        codeblock_length_max = np.NaN\n",
    "        \n",
    "        lists_unordered_num = np.NaN\n",
    "        lists_unordered_lengths = np.NaN\n",
    "        lists_unordered_items_sum = np.NaN\n",
    "        lists_unordered_items_median = np.NaN\n",
    "        lists_unordered_items_mean = np.NaN\n",
    "        lists_unordered_items_std = np.NaN\n",
    "        lists_unordered_items_min = np.NaN\n",
    "        lists_unordered_items_max = np.NaN\n",
    "\n",
    "        lists_ordered_num = np.NaN\n",
    "        lists_ordered_lengths = np.NaN\n",
    "        lists_ordered_items_sum = np.NaN\n",
    "        lists_ordered_items_median = np.NaN\n",
    "        lists_ordered_items_mean = np.NaN\n",
    "        lists_ordered_items_std = np.NaN\n",
    "        lists_ordered_items_min = np.NaN\n",
    "        lists_ordered_items_max = np.NaN\n",
    "        \n",
    "        paragraph_num  = np.NaN\n",
    "        italic_num = np.NaN\n",
    "        bold_num = np.NaN\n",
    "    return (text, sentiment_polarity, sentiment_subjectivity, word_count, TextSyllableNum, TextLexiconNum, TextSentenceNum, \n",
    "            ReadabilityFleschEase, ReadabilitySMOG, ReadabilityFleschKincaid, ReadabilityColemanLiau, ReadabilityARI, ReadabilityDaleChall, ReadabilityDifficultWordsList, ReadabilityDifficultWordsNum, ReadabilityLinsearWriteFormula, ReadabilityGunningFog, ReadabilityReadingTime, ReadabilityConsensus, \n",
    "            featured_image, code_inline_raw, code_inline_num, \n",
    "            codeblock_raw, codeblock_num, codeblock_lengths, codeblock_length_sum, codeblock_length_median, codeblock_length_mean, codeblock_length_std, codeblock_length_min, codeblock_length_max, \n",
    "            lists_ordered_num, lists_ordered_lengths, lists_ordered_items_sum, lists_ordered_items_median, lists_ordered_items_mean, lists_ordered_items_std, lists_ordered_items_min, lists_ordered_items_max,\n",
    "            lists_unordered_num, lists_unordered_lengths, lists_unordered_items_sum, lists_unordered_items_median, lists_unordered_items_mean, lists_unordered_items_std, lists_unordered_items_min, lists_unordered_items_max,\n",
    "            img_count, link_urls, link_count, highlights_text, highlight_count, paragraph_num, italic_num, bold_num)\n",
    "    \n",
    "# test = df\n",
    "# test[['Text', 'WordNum', 'HasFeaturedImage','CodeInlineRaw', 'CodeInlineNum', 'CodeBlockRaw', 'CodeBlockNum', \"CodeBlockLengthList\", \"CodeBlockLengthSum\", \"CodeBlockLengthMedian\", \"CodeBlockLengthMean\", \"CodeBlockLengthMin\", \"CodeBlockLengthMax\", 'ImgNum', 'LinkURLList', 'LinkNum', 'HLightTextList', 'HlightNum']] = df.apply(extract_features_from_html, axis=1, result_type=\"expand\")\n",
    "# del test['StoryHTML']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate feature extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 600 rows. Processed  190 clean rows\n",
      "600 rows read 190 rows clean\n",
      "Wall time: 7.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pd.options.display.max_colwidth = 10000\n",
    "filepath = 'data/df_story_features_full_Jul29.csv'\n",
    "# Remove csv if exists\n",
    "import os\n",
    "if os.path.exists(filepath):\n",
    "  os.remove(filepath)\n",
    "\n",
    "import extractFeaturesFunction\n",
    "\n",
    "count = 0\n",
    "header = True\n",
    "\n",
    "numRows = 600\n",
    "chunkSize =600\n",
    "\n",
    "\n",
    "\n",
    "processedRows = 0\n",
    "chunkRows = 0\n",
    " # Read chunks\n",
    "for data in pd.read_csv('data/raw/OneDrive - University Of Waterloo/df_story_chunk2.csv',encoding = 'ISO-8859-1', nrows=numRows, chunksize  = chunkSize, low_memory=False):\n",
    " # Read entire data\n",
    "# for data in pd.read_csv('data/df_story_chunk1.csv',encoding = 'ISO-8859-1', chunksize  = chunkSize, low_memory=False):\n",
    "\n",
    "    count += 1                          # counting the number of chunks\n",
    "    lastlen = len(data)                 # finding the length of last chunk\n",
    "\n",
    "    # Filter NaN PostID\n",
    "    data = data[-data['PostID'].isnull()]      \n",
    "    \n",
    "    # Filter NaN ResponseTime\n",
    "    data = data[-data['ReadingTime'].isnull()] \n",
    "\n",
    "    # Filter stories posted after April 2020\n",
    "    data['PublishedDate'] = pd.to_datetime(data['PublishedDate']).dt.date\n",
    "    data = data[data['PublishedDate'] < pd.to_datetime(\"2020-04-01\")]\n",
    "\n",
    "    \n",
    "    # Convert responsetime \"X min read\" to X as int\n",
    "    data['ReadingTime'] = data['ReadingTime'].str.extract('(\\d+)', expand=False).astype(int) \n",
    "    \n",
    "    # Get number of tags used\n",
    "    data['TagNum'] = data[['Tag1','Tag2','Tag3','Tag4', 'Tag5']].notnull().sum(axis=1)\n",
    "\n",
    "    # Boolean, if article belongs to publication\n",
    "    data['isPublication'] = data['Company'].notnull()\n",
    "    \n",
    "    # Extract features  from HTML\n",
    "#     data[['Text', 'SentimentPolarity', 'SentimentSubjectivity', 'WordNum', \"TextSyllableNum\",\n",
    "#           \"TextLexiconNum\", \"TextSentenceNum\", \"ReadabilityFleschEase\", \"ReadabilitySMOG\", \"ReadabilityFleschKincaid\", \"ReadabilityColemanLiau\", \"ReadabilityARI\", \"ReadabilityDaleChall\", \"ReadabilityDifficultWordsList\", \"ReadabilityDifficultWordsNum\", \"ReadabilityLinsearWriteFormula\", \"ReadabilityGunningFog\", \"ReadabilityReadingTime\", \"ReadabilityConsensus\", \n",
    "#           'HasFeaturedImage','CodeInlineRaw', 'CodeInlineNum', \n",
    "#           'CodeBlockRaw', 'CodeBlockNum', \"CodeBlockLengthList\", \"CodeBlockLengthSum\", \"CodeBlockLengthMedian\", \"CodeBlockLengthMean\", \"CodeBlockLengthStd\", \"CodeBlockLengthMin\", \"CodeBlockLengthMax\",\n",
    "#           'ListOlNum', 'ListOlLength', 'ListOlSum', 'ListOlMedian', 'ListOlMean', 'ListOlStd', \"ListOlMin\", 'ListOlMax',\n",
    "#           'ListUlNum', 'ListUlLength', 'ListUlSum', 'ListUlMedian', 'ListUlMean', 'ListUlStd', \"ListUlMin\", 'ListUlMax',\n",
    "#           'ImgNum', 'LinkURLList', 'LinkNum', 'HLightTextList', 'HlightNum', 'ParagraphNum', \"ItalicNum\", \"BoldNum\"]] = data.apply(extract_features_from_html, axis=1, result_type=\"expand\")\n",
    "\n",
    "    data[['Text', 'SentimentPolarity', 'SentimentSubjectivity', 'WordNum', \"TextSyllableNum\",\n",
    "          \"TextLexiconNum\", \"TextSentenceNum\", \"ReadabilityFleschEase\", \"ReadabilitySMOG\", \"ReadabilityFleschKincaid\", \"ReadabilityColemanLiau\", \"ReadabilityARI\", \"ReadabilityDaleChall\", \"ReadabilityDifficultWordsList\", \"ReadabilityDifficultWordsNum\", \"ReadabilityLinsearWriteFormula\", \"ReadabilityGunningFog\", \"ReadabilityReadingTime\", \"ReadabilityConsensus\", \n",
    "          'HasFeaturedImage','CodeInlineRaw', 'CodeInlineNum', \n",
    "          'CodeBlockRaw', 'CodeBlockNum', \"CodeBlockLengthList\", \"CodeBlockLengthSum\", \"CodeBlockLengthMedian\", \"CodeBlockLengthMean\", \"CodeBlockLengthStd\", \"CodeBlockLengthMin\", \"CodeBlockLengthMax\",\n",
    "          'ListOlNum', 'ListOlLength', 'ListOlSum', 'ListOlMedian', 'ListOlMean', 'ListOlStd', \"ListOlMin\", 'ListOlMax',\n",
    "          'ListUlNum', 'ListUlLength', 'ListUlSum', 'ListUlMedian', 'ListUlMean', 'ListUlStd', \"ListUlMin\", 'ListUlMax',\n",
    "          'ImgNum', 'LinkURLList', 'LinkNum', 'HLightTextList', 'HlightNum', 'ParagraphNum', \"ItalicNum\", \"BoldNum\"]] = pd.DataFrame(extractFeaturesFunction.parallelize_on_rows(data, extractFeaturesFunction.extract_features_from_html_multiproc).tolist())\n",
    "\n",
    "#     print(type(mypd))\n",
    "    # Join author data\n",
    "    data = pd.merge(data, authorStats, on=\"User\")   \n",
    "    \n",
    "    # Join publication stats\n",
    "    \n",
    "    # PublicationClapCount: number of articles that the publication contains\n",
    "    data = pd.merge(data, tagStats1, on=\"Tag1\", how='left')\n",
    "    data = pd.merge(data, tagStats2, on=\"Tag2\", how='left')\n",
    "    data = pd.merge(data, tagStats3, on=\"Tag3\", how='left')\n",
    "    data = pd.merge(data, tagStats4, on=\"Tag4\", how='left')\n",
    "    data = pd.merge(data, tagStats5, on=\"Tag5\", how='left')\n",
    "\n",
    "    # Join publication data\n",
    "    data = pd.merge(data, publicationStats, on=\"Company\", how='left').fillna(0)\n",
    "    \n",
    "    # Sum tag uses for all tags\n",
    "    \n",
    "    # TagUseSum: total number of times that Tag1,2,3,4,5 have been used\n",
    "    # TagUseMean: mean usage number of Tag1,2,3,4,5\n",
    "    # TagUseMedian, median usage number of Tag1,2,3,4,5\n",
    "    \n",
    "    # TagClapSum: total number of claps of articles in Tag1,2,3,4,5    \n",
    "    data['TagUseSum'] = data['Tag1Clap_count'].fillna(0) + data['Tag2Clap_count'].fillna(0) + data['Tag3Clap_count'].fillna(0) + data['Tag4Clap_count'].fillna(0) + data['Tag5Clap_count'].fillna(0)\n",
    "    data['TagUseMean'] = data['TagUseSum'].fillna(0)/data['TagNum'].fillna(1)\n",
    "    data['TagUseMedian'] = data[['Tag1Clap_count', 'Tag2Clap_count', 'Tag3Clap_count', 'Tag4Clap_count', 'Tag5Clap_count']].median(axis=1)\n",
    "    \n",
    "    data['TagClapSum']  = data['Tag1Clap_sum'].fillna(0) + data['Tag2Clap_sum'].fillna(0) + data['Tag3Clap_sum'].fillna(0) + data['Tag4Clap_sum'].fillna(0) + data['Tag5Clap_sum'].fillna(0)\n",
    "    data['TagClapMean'] = data['TagClapSum'].fillna(0)/data['TagNum'].fillna(1)\n",
    "    data['TagClapMedian'] = data[['Tag1Clap_sum', 'Tag2Clap_sum', 'Tag3Clap_sum', 'Tag4Clap_sum', 'Tag5Clap_sum']].median(axis=1)\n",
    "\n",
    "    data['TagUseMean'].fillna(0, inplace=True)\n",
    "    data['TagUseMedian'].fillna(0, inplace=True)\n",
    "    data['TagClapMean'].fillna(0, inplace=True)\n",
    "    data['TagClapMedian'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Delete raw html\n",
    "    del data['StoryHTML']\n",
    "    \n",
    "    processedRows += len(data)\n",
    "    chunkRows += chunkSize\n",
    "    print(\"Processed\", chunkRows, \"rows. Processed \", processedRows, \"clean rows\")\n",
    "    \n",
    "    data.to_csv(filepath, header=header, mode=\"a\")\n",
    "    header = False\n",
    "datalength = (count*chunkSize + lastlen - chunkSize) # length of total file\n",
    "print(datalength, \"rows read\", processedRows, \"rows clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ClapCount_Card</th>\n",
       "      <th>ClapCount_Story</th>\n",
       "      <th>Company</th>\n",
       "      <th>CompanyURL</th>\n",
       "      <th>PostID</th>\n",
       "      <th>PublishedDate</th>\n",
       "      <th>ReadingTime</th>\n",
       "      <th>ResponseNum_Card</th>\n",
       "      <th>ResponseNum_Story</th>\n",
       "      <th>StoryIndex</th>\n",
       "      <th>StoryTitle</th>\n",
       "      <th>StoryURL</th>\n",
       "      <th>StoryURL_Story</th>\n",
       "      <th>Tag1</th>\n",
       "      <th>Tag2</th>\n",
       "      <th>Tag3</th>\n",
       "      <th>Tag4</th>\n",
       "      <th>Tag5</th>\n",
       "      <th>TagSource</th>\n",
       "      <th>User</th>\n",
       "      <th>UserURL</th>\n",
       "      <th>VoterCount</th>\n",
       "      <th>isPaywall</th>\n",
       "      <th>TagNum</th>\n",
       "      <th>isPublication</th>\n",
       "      <th>Text</th>\n",
       "      <th>SentimentPolarity</th>\n",
       "      <th>SentimentSubjectivity</th>\n",
       "      <th>WordNum</th>\n",
       "      <th>TextSyllableNum</th>\n",
       "      <th>TextLexiconNum</th>\n",
       "      <th>TextSentenceNum</th>\n",
       "      <th>ReadabilityFleschEase</th>\n",
       "      <th>ReadabilitySMOG</th>\n",
       "      <th>ReadabilityFleschKincaid</th>\n",
       "      <th>ReadabilityColemanLiau</th>\n",
       "      <th>ReadabilityARI</th>\n",
       "      <th>ReadabilityDaleChall</th>\n",
       "      <th>ReadabilityDifficultWordsList</th>\n",
       "      <th>ReadabilityDifficultWordsNum</th>\n",
       "      <th>ReadabilityLinsearWriteFormula</th>\n",
       "      <th>ReadabilityGunningFog</th>\n",
       "      <th>ReadabilityReadingTime</th>\n",
       "      <th>ReadabilityConsensus</th>\n",
       "      <th>HasFeaturedImage</th>\n",
       "      <th>CodeInlineRaw</th>\n",
       "      <th>CodeInlineNum</th>\n",
       "      <th>CodeBlockRaw</th>\n",
       "      <th>CodeBlockNum</th>\n",
       "      <th>CodeBlockLengthList</th>\n",
       "      <th>CodeBlockLengthSum</th>\n",
       "      <th>CodeBlockLengthMedian</th>\n",
       "      <th>CodeBlockLengthMean</th>\n",
       "      <th>CodeBlockLengthStd</th>\n",
       "      <th>CodeBlockLengthMin</th>\n",
       "      <th>CodeBlockLengthMax</th>\n",
       "      <th>ListOlNum</th>\n",
       "      <th>ListOlLength</th>\n",
       "      <th>ListOlSum</th>\n",
       "      <th>ListOlMedian</th>\n",
       "      <th>ListOlMean</th>\n",
       "      <th>ListOlStd</th>\n",
       "      <th>ListOlMin</th>\n",
       "      <th>ListOlMax</th>\n",
       "      <th>ListUlNum</th>\n",
       "      <th>ListUlLength</th>\n",
       "      <th>ListUlSum</th>\n",
       "      <th>ListUlMedian</th>\n",
       "      <th>ListUlMean</th>\n",
       "      <th>ListUlStd</th>\n",
       "      <th>ListUlMin</th>\n",
       "      <th>ListUlMax</th>\n",
       "      <th>ImgNum</th>\n",
       "      <th>LinkURLList</th>\n",
       "      <th>LinkNum</th>\n",
       "      <th>HLightTextList</th>\n",
       "      <th>HlightNum</th>\n",
       "      <th>ParagraphNum</th>\n",
       "      <th>ItalicNum</th>\n",
       "      <th>BoldNum</th>\n",
       "      <th>AuthorArticlesClap_mean</th>\n",
       "      <th>AuthorArticlesClap_median</th>\n",
       "      <th>AuthorArticlesClap_count</th>\n",
       "      <th>AuthorArticlesClap_sum</th>\n",
       "      <th>AuthorArticlesResponse_mean</th>\n",
       "      <th>AuthorArticlesResponse_median</th>\n",
       "      <th>AuthorArticlesResponse_count</th>\n",
       "      <th>AuthorArticlesResponse_sum</th>\n",
       "      <th>AuthorArticlesVoter_mean</th>\n",
       "      <th>AuthorArticlesVoter_median</th>\n",
       "      <th>AuthorArticlesVoter_count</th>\n",
       "      <th>AuthorArticlesVoter_sum</th>\n",
       "      <th>Tag1Clap_mean</th>\n",
       "      <th>Tag1Clap_median</th>\n",
       "      <th>Tag1Clap_count</th>\n",
       "      <th>Tag1Clap_sum</th>\n",
       "      <th>Tag1Clap_std</th>\n",
       "      <th>Tag1Response_mean</th>\n",
       "      <th>Tag1Response_median</th>\n",
       "      <th>Tag1Response_count</th>\n",
       "      <th>Tag1Response_sum</th>\n",
       "      <th>Tag1Response_std</th>\n",
       "      <th>Tag1ReadingTime_mean</th>\n",
       "      <th>Tag1ReadingTime_median</th>\n",
       "      <th>Tag1ReadingTime_count</th>\n",
       "      <th>Tag1ReadingTime_sum</th>\n",
       "      <th>Tag1ReadingTime_std</th>\n",
       "      <th>Tag2Clap_mean</th>\n",
       "      <th>Tag2Clap_median</th>\n",
       "      <th>Tag2Clap_count</th>\n",
       "      <th>Tag2Clap_sum</th>\n",
       "      <th>Tag2Clap_std</th>\n",
       "      <th>Tag2Response_mean</th>\n",
       "      <th>Tag2Response_median</th>\n",
       "      <th>Tag2Response_count</th>\n",
       "      <th>Tag2Response_sum</th>\n",
       "      <th>Tag2Response_std</th>\n",
       "      <th>Tag2ReadingTime_mean</th>\n",
       "      <th>Tag2ReadingTime_median</th>\n",
       "      <th>Tag2ReadingTime_count</th>\n",
       "      <th>Tag2ReadingTime_sum</th>\n",
       "      <th>Tag2ReadingTime_std</th>\n",
       "      <th>Tag3Clap_mean</th>\n",
       "      <th>Tag3Clap_median</th>\n",
       "      <th>Tag3Clap_count</th>\n",
       "      <th>Tag3Clap_sum</th>\n",
       "      <th>Tag3Clap_std</th>\n",
       "      <th>Tag3Response_mean</th>\n",
       "      <th>Tag3Response_median</th>\n",
       "      <th>Tag3Response_count</th>\n",
       "      <th>Tag3Response_sum</th>\n",
       "      <th>Tag3Response_std</th>\n",
       "      <th>Tag3ReadingTime_mean</th>\n",
       "      <th>Tag3ReadingTime_median</th>\n",
       "      <th>Tag3ReadingTime_count</th>\n",
       "      <th>Tag3ReadingTime_sum</th>\n",
       "      <th>Tag3ReadingTime_std</th>\n",
       "      <th>Tag4Clap_mean</th>\n",
       "      <th>Tag4Clap_median</th>\n",
       "      <th>Tag4Clap_count</th>\n",
       "      <th>Tag4Clap_sum</th>\n",
       "      <th>Tag4Clap_std</th>\n",
       "      <th>Tag4Response_mean</th>\n",
       "      <th>Tag4Response_median</th>\n",
       "      <th>Tag4Response_count</th>\n",
       "      <th>Tag4Response_sum</th>\n",
       "      <th>Tag4Response_std</th>\n",
       "      <th>Tag4ReadingTime_mean</th>\n",
       "      <th>Tag4ReadingTime_median</th>\n",
       "      <th>Tag4ReadingTime_count</th>\n",
       "      <th>Tag4ReadingTime_sum</th>\n",
       "      <th>Tag4ReadingTime_std</th>\n",
       "      <th>Tag5Clap_mean</th>\n",
       "      <th>Tag5Clap_median</th>\n",
       "      <th>Tag5Clap_count</th>\n",
       "      <th>Tag5Clap_sum</th>\n",
       "      <th>Tag5Clap_std</th>\n",
       "      <th>Tag5Response_mean</th>\n",
       "      <th>Tag5Response_median</th>\n",
       "      <th>Tag5Response_count</th>\n",
       "      <th>Tag5Response_sum</th>\n",
       "      <th>Tag5Response_std</th>\n",
       "      <th>Tag5ReadingTime_mean</th>\n",
       "      <th>Tag5ReadingTime_median</th>\n",
       "      <th>Tag5ReadingTime_count</th>\n",
       "      <th>Tag5ReadingTime_sum</th>\n",
       "      <th>Tag5ReadingTime_std</th>\n",
       "      <th>PublicationClap_mean</th>\n",
       "      <th>PublicationClap_median</th>\n",
       "      <th>PublicationClap_count</th>\n",
       "      <th>PublicationClap_sum</th>\n",
       "      <th>PublicationReadingTime_mean</th>\n",
       "      <th>PublicationReadingTime_median</th>\n",
       "      <th>PublicationReadingTime_count</th>\n",
       "      <th>PublicationReadingTime_sum</th>\n",
       "      <th>PublicationVoter_mean</th>\n",
       "      <th>PublicationVoter_median</th>\n",
       "      <th>PublicationVoter_count</th>\n",
       "      <th>PublicationVoter_sum</th>\n",
       "      <th>PublicationisPaywall_mean</th>\n",
       "      <th>PublicationisPaywall_median</th>\n",
       "      <th>PublicationisPaywall_count</th>\n",
       "      <th>PublicationisPaywall_sum</th>\n",
       "      <th>TagUseSum</th>\n",
       "      <th>TagUseMean</th>\n",
       "      <th>TagUseMedian</th>\n",
       "      <th>TagClapSum</th>\n",
       "      <th>TagClapMean</th>\n",
       "      <th>TagClapMedian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2b519cdbf19e</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15894</td>\n",
       "      <td>offers you a lot- the possibilities are endless with it.</td>\n",
       "      <td>https://medium.com/@seo.navicosoft.com/web-hosting-2b519cdbf19e?source=tag_archive---------0-----------------------</td>\n",
       "      <td>https://medium.com/@seo.navicosoft.com/web-hosting-2b519cdbf19e?source=tag_archive---------0-----------------------</td>\n",
       "      <td>web-hosting</td>\n",
       "      <td>web-hosting-company</td>\n",
       "      <td>web-hosting-services</td>\n",
       "      <td>features-of-good-host</td>\n",
       "      <td>0</td>\n",
       "      <td>web-hosting-services</td>\n",
       "      <td>Navicosoft</td>\n",
       "      <td>https://medium.com/@seo.navicosoft.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.492967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3057.0</td>\n",
       "      <td>16792.0</td>\n",
       "      <td>105.228047</td>\n",
       "      <td>0.376513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3057.0</td>\n",
       "      <td>1151.0</td>\n",
       "      <td>3.670922</td>\n",
       "      <td>0.004253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3057.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.065083</td>\n",
       "      <td>5.638298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>37.761509</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.196279</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>4.47323</td>\n",
       "      <td>0.093168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.367445</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.078811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3265.0</td>\n",
       "      <td>816.25</td>\n",
       "      <td>47.0</td>\n",
       "      <td>17148.0</td>\n",
       "      <td>4287.0</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1.5K</td>\n",
       "      <td>1571.0</td>\n",
       "      <td>OneZero</td>\n",
       "      <td>https://onezero.medium.com?source=tag_archive---------7-----------------------</td>\n",
       "      <td>b9f612475080</td>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>5</td>\n",
       "      <td>10 responses</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7209</td>\n",
       "      <td>Google Is Finally Copying Appleâs Approach toÂ Privacy</td>\n",
       "      <td>https://onezero.medium.com/google-is-finally-copying-apples-approach-to-privacy-b9f612475080?source=tag_archive---------7-----------------------#--responses</td>\n",
       "      <td>https://onezero.medium.com/google-is-finally-copying-apples-approach-to-privacy-b9f612475080?source=tag_archive---------7-----------------------&amp;gi=sd</td>\n",
       "      <td>google</td>\n",
       "      <td>industry</td>\n",
       "      <td>privacy</td>\n",
       "      <td>consumer-tech</td>\n",
       "      <td>google-io-2019</td>\n",
       "      <td>google-io-2019</td>\n",
       "      <td>Eric Ravenscraft</td>\n",
       "      <td>https://onezero.medium.com/@lordravenscraft_50708</td>\n",
       "      <td>317.0</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1130.53125</td>\n",
       "      <td>903.5</td>\n",
       "      <td>32</td>\n",
       "      <td>36177.0</td>\n",
       "      <td>185.71875</td>\n",
       "      <td>128.5</td>\n",
       "      <td>32</td>\n",
       "      <td>5943.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.080320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28374.0</td>\n",
       "      <td>711629.0</td>\n",
       "      <td>1081.548688</td>\n",
       "      <td>3.423063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28374.0</td>\n",
       "      <td>97126.0</td>\n",
       "      <td>103.633636</td>\n",
       "      <td>0.018573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28374.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>0.135015</td>\n",
       "      <td>22.723109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2380.0</td>\n",
       "      <td>54081.0</td>\n",
       "      <td>243.463635</td>\n",
       "      <td>2.438655</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2380.0</td>\n",
       "      <td>5804.0</td>\n",
       "      <td>24.959909</td>\n",
       "      <td>0.04958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2380.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.217121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.705882</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>62.693465</td>\n",
       "      <td>4.470588</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>6.094235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2437.666667</td>\n",
       "      <td>2859.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7313.0</td>\n",
       "      <td>750.651273</td>\n",
       "      <td>493.333333</td>\n",
       "      <td>491.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>177.511502</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2166.97929</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>1464878.0</td>\n",
       "      <td>6.863905</td>\n",
       "      <td>6.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>4640.0</td>\n",
       "      <td>336.739645</td>\n",
       "      <td>143.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>227636.0</td>\n",
       "      <td>0.997041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>674.0</td>\n",
       "      <td>30774.0</td>\n",
       "      <td>6154.80</td>\n",
       "      <td>17.0</td>\n",
       "      <td>773613.0</td>\n",
       "      <td>154722.6</td>\n",
       "      <td>7313.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = pd.read_csv(\"data/df_story_features_full_Jul29.csv\")\n",
    "display(HTML(test.sample(2).to_html()))\n",
    "# del test['Text']\n",
    "# del test['CodeBlockRaw']\n",
    "# print(test[['Tag5', 'Tag5Clap_sum', 'TagUseSum']].sample())\n",
    "# display(HTML(test.sample(20).to_html()))\n",
    "# test.head()\n",
    "# display(test['PublishedDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(HTML(test.merge(authorStats[['User', 'ClapCount_Story_count']], on=\"User\").sample(5).to_html()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_html = df.iloc[547].StoryHTML\n",
    "testsoup = BeautifulSoup(test_html, 'lxml')\n",
    "print(testsoup.find_all(\"figure\", {'class':re.compile(\"/.*[^image]$/\")}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(HTML(df.head(5).to_html()))\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "print(df[df['CodeNum']>0].head(100)['StoryURL'].to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(df[df['CodeBlockLengthMean']>0]['CodeBlockLengthMean'].mean())\n",
    "df['CodeBlockLengthMean'].hist(bins=60)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
